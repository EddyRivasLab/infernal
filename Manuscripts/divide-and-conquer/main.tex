\documentclass[11pt]{article}
\usepackage{times}
\newif\ifpdf
\ifx\pdfoutput\undefined
  \pdffalse
  \usepackage[dvips]{graphicx}    
  \usepackage[dvips]{color}  
\else
  \pdftrue
  \usepackage[pdftex]{graphicx}    
  \usepackage[pdftex]{color}  
\fi
\usepackage{fullpage}
\usepackage{url}
\usepackage{cite}
\usepackage{alg}
\bibliographystyle{bmc_bioinformatics}

\setcounter{secnumdepth}{0}

\begin{document}

\title{A memory-efficient algorithm for optimally aligning an RNA sequence to a
secondary structure consensus model}
\author{Sean R. Eddy\\
Howard Hughes Medical Institute \& Department of Genetics, \\
Washington University School of Medicine \\
Saint Louis, Missouri 63110 USA \\
eddy@genetics.wustl.edu}  
\date{\today}
\maketitle

\def\argmax{\mbox{\rm{argmax}}}      % argmax
\def\argmin{\mbox{\rm{argmin}}}      % argmin

\abstract{The problem of optimally aligning an RNA sequence to an RNA
secondary structure consensus model is considered. Stochastic
context-free grammars (SCFGs) provide a probabilistic framework for
modeling both sequence conservation and long-distance base pairing
correlations that are characteristic of RNA secondary structure
consensus. Covariance models (CMs), a restricted case of SCFGs
specifically suited to modeling RNA structure consensus, have been
previously described. The standard CYK algorithm for optimal dynamic
programming alignment of a CM/SCFG to an RNA sequence requires roughly
$O(N^3)$ (cubic) memory for an RNA of length $N$. This is only
practical for small RNAs of a few hundred nucleotides or less. Here I
describe a divide and conquer variant of the CM/SCFG alignment
algorithm, analogous to the memory-efficient Myers/Miller form of the
Smith/Waterman algorithm for linear sequence alignment. The algorithm
requires roughly $O(N^2 \log N)$ memory, at the expense of a small
constant factor in time. Small subunit ribosomal RNA alignments that
previously required a prohibitive 20-30 GB of memory to solve by
dynamic programming now require $\approx$ 80 MB and are well within
the capabilities of most modern computers.}

\section{Introduction}

Given a structural RNA of known secondary structure, I want to
identify homologous structural RNAs in a sequence database. Because
RNAs often tend to conserve a base-paired secondary structure more
than they conserve primary sequence, it would be best if database
searches for homologous RNAs took into account both the primary and
secondary structure of the query RNA.

Some excellent approaches already exist in the literature. For
example, exact- and approximate-match pattern searches (a la PROSITE
patterns) can and have been extended to allow patterns to specify
long-range base pairing constraints. Three such programs are PATSCAN,
RNAMOT, and RNABOB. Another approach involves developing a specialized
program to recognize a specific type of RNA -- for example, several
programs exist for detecting transfer RNA genes, one for group I
catalytic introns, and several groups, including mine, have recently
used computational searches for new small nucleolar RNAs.

In primary sequence analysis, a technologically more mature field, the
main techniques are neither pattern searches nor specialized
programs. The main techniques are sequence alignment algorithms with
probabilistically-based scoring systems -- for example, the BLAST or
FASTA algorithms, and the PAM or BLOSUM score matrices. Unlike
specialized programs, a general alignment algorithm can be applied to
any query sequence. Unlike pattern searches, which give yes/no answers
for whether a candidate sequence is a match, a scoring system gives a
meaningful score that allows ranking candidate hits by their
statistical significance. It is thus of interest to develop general
alignment algorithms for structural RNA queries.

Eddy and Durbin and Sakakibara \emph{et al.} introduced stochastic
context free grammar (SCFG) algorithms as a general approach to RNA
alignment \cite{Eddy94,Sakakibara94c,Durbin98}. SCFGs allow the strong
pairwise residue correlations in non-pseudoknotted RNA secondary
structure to be taken into account in pairwise or profile RNA
alignments, while using a dynamic programming algorithm that
guarantees finding a mathematically optimal alignment in polynomial
time. SCFG alignment algorithms are a natural extension of standard
linear sequence alignment algorithms (particularly those with fully
probabilistic, hidden Markov model formulations) into an additional
dimension necessary to deal with 2D RNA secondary structure.

While SCFGs may provide a natural mathematical framework for RNA
secondary structure alignment problems \cite{Durbin98}, SCFG
algorithms have high computational complexity. Optimal SCFG-based
structural alignment of an RNA structure to a sequence costs $O(N^3)$
memory and $O(N^4)$ time for a sequence of length $N$, compared to
$O(N^2)$ memory and time for sequence alignment algorithms. Corpet and
Michot have described RNAlign, a program that also implements a
different general dynamic programming algorithm for RNA alignment; the
RNAlign algorithm solves the same problem as SCFGs even less
efficiently, requiring $O(N^5)$ time and $O(N^4)$ memory.  SCFG-based
alignments of small structural RNAs are feasible. Using my COVE
software, transfer RNA alignments (~75 nucleotides) take about 1 cpu
second and 3 Mb of memory. Most genome centers now use an SCFG-based
search program, tRNAscan-SE, for annotating transfer RNA genes
\cite{LoweEddy97}. However, many larger RNAs of interest are outside
the capabilities of published SCFG alignment algorithms. Alignment of
a small subunit (SSU) ribosomal RNA sequence to the SSU rRNA consensus
structure would take about 23 Gb of RAM and 8 hours of CPU time. The
steep memory requirement is particularly galling. It constitutes a
significant practical barrier to the wider applicability of SCFG
algorithms.

% The above figures come from INFERNAL with manually set L.
% cmbuild --rf foo.cm tRNA1415.sto:
%    M=233 L=75 
%    Full CYK = 2.7 Mb. 822206 cycles.  ~ 1 sec.
%    Small = 0.14 Mb.
% cmbuild foo.cm ssu.sto
%    M=4789 L=1542
%    Full CYK = 22.7 Gb. 2.4e10 cycles. ~ 8 hr.
%    Small = 86 Mb.

Notredame \emph{et al.} have pointed specifically at this problem.
They described RAGA \cite{Notredame97}, a program that uses a genetic
algorithm (GA) to optimize a pairwise RNA alignment using an objective
function that includes base pairing terms. Because GAs have an $O(N)$
memory requirement, RAGA can effectively find reasonable solutions for
large RNA alignment problems (such as ribosomal RNA
alignments). Although RAGA does not guarantee a mathematically optimal
solution, it is probably the current state of the art for structural
alignment of large RNAs.

Here, I introduce a dynamic programming solution to the problem of
structural alignment of large RNAs. The central idea is a ``divide and
conquer'' strategy. There is a historical parallel.  For linear
sequence alignment, a divide and conquer algorithm was introduced by
Hirschberg \cite{Hirschberg75}, an algorithm better known in the
computational biology community as the Myers/Miller algorithm
\cite{MyM-88a}.  Ironically, at the time, dynamic programming methods
for optimal sequence alignment were well known, but were considered
impractical on 1970's era computers because of an ``extreme'' $O(N^2)$
memory requirement. Myers/Miller reduces the memory complexity of a
dynamic programming sequence alignment algorithm from $O(N^2)$ to
$O(N)$, at the cost of a constant (roughly two-fold) increase in CPU
time. Here I show that a divide and conquer strategy can also be
applied to a three dimensional SCFG dynamic programming matrix,
greatly reducing the memory requirement of SCFG alignments and making
optimal structural alignment of large RNAs possible on our current
generation of computers.

I will strictly be dealing with the problem of aligning a target
sequence of unknown structure to a query RNA of known secondary
structure. By ``secondary structure'' I mean nested (nonpseudoknotted)
pairwise RNA secondary structure interactions, primarily Watson-Crick
but also allowing other noncanonical RNA base pairs. I refer to this
as an RNA structural alignment problem to distinguish it from linear
sequence alignment. I am not addressing the problem of structurally
superposing molecules of known three-dimensional structure, nor the
problem of structurally aligning known RNA secondary structures to
each other, nor the problem of trying to simultaneously infer both a
consensus secondary structure and a secondary structure alignment for
a set of unaligned RNA sequences of unknown structure.

\section{Algorithm}

\subsection{Overview}

There are three key ideas behind a divide and conquer dynamic
programming method.

First is the fact that if all we need is the score of the optimal
alignment, not the alignment itself, the whole DP matrix need not be
kept in memory. For example, in primary sequence alignment, the
iterative calculation of each cell in a given row of a two-dimensional
DP matrix depends only on values in cells in that row and the
immediately preceding row, so a score-only calculation may be done in
$O(N)$ space.

Second is the fact that the fill stage of DP algorithms may be run
both forwards and backwards. For instance, in primary sequence
alignment, one usually iteratively calculates the optimal score
$F(i,j)$ of the best alignment of the prefix $1..i$ of sequence 1
aligned to the prefix $1..j$ of sequence 2, for increasing $i$ and
$j$, until one obtains $F(N,M)$, the score of the optimal alignment of
all of sequence 1 (length $N$) to sequence 2 (length $M$). One can
also just as easily calculate the optimal score $B(i,j)$ of the best
alignment of the
\emph{suffix} $i+1..N$ of sequence 1 to the suffix $j+1..M$ of sequence 2,
until one obtains $B(0,0)$ -- a number that will be identical to
$F(N,M)$.

Third is the fact that the sum of the forwards and backwards numbers
at any cell in the optimal path though the DP matrix is equal to the
optimal overall alignment score. In linear sequence alignment I can
pick a ``split point'' row $i'$ in the sequence; run the forward
calculation to $i'$ to obtain $F(i',j)$ numbers; run the backwards
calculation back to $i'$ to get $B(i',j)$ numbers; then find
$\argmax_j F(i',j) + B(i',j)$, which tells me that the optimal
alignment passes through cell (i',j). (I am leaving out some important
details of how indels and local alignments are handled.) This divides
the alignment into two smaller alignment problems. The complete
alignment can be found by a recursive series of split point
calculations. Although this seems laborious -- recovering only a
single point in the alignment per step -- the size of the DP problems
is decreased by 4-fold at each split. A complete alignment costs only
about $1 + 2 \times \frac{1}{4} + 4 \times \frac{1}{16} + \ldots
\simeq 2$ times as much CPU time as the single step DP calculation,
but every step in the recursive divide-and-conquer calculation cost
only $O(N)$ memory.

For SCFG alignment, the standard Cocke-Younger-Kasami (referred to in
this paper as Inside/CYK) alignment algorithm can also be run in a
memory-saving ``score only'' mode, as can the backwards analogue,
Outside/CYK. The Inside/Outside splitting procedure one needs for an
SCFG divide and conquer is not as obvious as it is for
forwards/backwards splitting in sequence alignment, but it is still
possible.

Rather than showing generalized memory-efficient SCFG alignment
algorithms, here I will describe algorithms specific for ``RNA
covariance models'' (CMs), a specialized form of SCFG suited for RNA
secondary structure alignment. My divide and conquer algorithm takes
advantage of features of CMs and does not apply to SCFGs in
general. Therefore I need to start with an introduction to what CMs
are, how they correspond to a known RNA secondary structure, and how
they are built and parameterized.
      
\subsection{Definition and construction of a covariance model}

A covariance model is a restricted form of stochastic context free
grammar that is well suited for modeling a consensus RNA secondary
structure.

\subsubsection{Definition of a stochastic context free grammar}

A stochastic context free grammar (SCFG) consists of the following:

\begin{itemize}
\item $M$ nonterminals (here called \emph{states}).
\item $K$ terminal symbols (e.g. the observable alphabet, {A,C,G,U} for RNA)
\item a number of \emph{production rules} of the form:
      $V \rightarrow \alpha$,
      where $\alpha$ can be any string of nonterminal and/or terminal
      symbols, including the empty string
      $\epsilon$.
\item Each production rule is associated with a probability, such that
      the sum of the production probabilities for any given
      nonterminal $V$ is equal to 1.
\end{itemize} 

\subsubsection{Relationship between SCFGs and covariance models}

A covariance model is a specific repetitive SCFG architecture
consisting of groups of model states that are associated with base
pairs and single-stranded positions in an RNA secondary structure
consensus. A covariance model has nine types of states with seven
kinds of production rules:

\begin{tabular}{lllll}
State  & Description             &  Production             & Emission & Transition\\ \hline
MP  & (pair emitting)           & $V_P \rightarrow a Y b$ & $e_v(a,b)$ & $t_v(y)$  \\
ML, IL & (left emitting)       & $V_L \rightarrow a Y$   & $e_v(a)$   & $t_v(y)$  \\
MR, IR & (right emitting)      & $V_R \rightarrow Y a$   & $e_v(a)$   & $t_v(y)$  \\
B &  (bifurcation)    & $V_B \rightarrow Y_S Z_S$  & 1     &     1     \\ \hline
D & (delete)         & $V_D \rightarrow Y$     &    1     &   $t_v(y)$  \\
S & (start)          & $V_S \rightarrow Y$     &    1     &   $t_v(y)$  \\
E & (end)            & $V_E \rightarrow \epsilon$ & 1     &     1     \\
\end{tabular}

For example, a consensus (``match'') pair (MP) state produces two
correlated letters a and b and transits to one of several possible new
states Y of various types.  A bifurcation (B) state splits into two
new start (S) states Y$_{S}$ and Z$_{S}$ with probability 1.  The E
state is a special case that terminates a derivation ($\epsilon$
represents the empty string). The reason for separate ML and IL states
instead of just L is to distinguish between consensus (``match'')
states and non-consensus ``insert'' states.

Each overall production probability is the product of an emission
probability $e_v$ and a transition probability $t_v$. This is an
independence assumption. The analogous assumption is made by hidden
Markov models, which are a special case of stochastic regular
grammars.

\subsubsection{From consensus structural alignment to guide tree}

Figure~\ref{fig:input_alignment} shows an example input file: a
multiple sequence alignment of homologous RNAs, with a line describing
the consensus RNA secondary structure. The first step of building a CM
is to produce a binary \emph{guide tree} of \emph{nodes} representing
the consensus secondary structure. The guide tree is a parse tree for
the consensus structure, with nodes as nonterminals and alignment
columns as terminals.

The guide tree has eight types of nodes:

\begin{tabular}{lll}
Node      & Description      &  Production           \\ \hline
MATP  & (pair)                 & $V_{\mbox{\tiny MATP}} \rightarrow a Y b$  \\
MATL  & (single strand, left)  & $V_{\mbox{\tiny MATL}} \rightarrow a Y$   \\
MATR  & (single strand, right) & $V_{\mbox{\tiny MATR}} \rightarrow Y a$   \\
BIF   & (bifurcation)          & $V_{\mbox{\tiny BIF}}  \rightarrow
Y_{\mbox{\tiny BEGL}} Z_{\mbox{\tiny BEGR}}$ \\
ROOT  & (root)                 & $V_{\mbox{\tiny ROOT}} \rightarrow Y$       \\
BEGL  & (begin, left)          & $V_{\mbox{\tiny BEGL}} \rightarrow Y$       \\
BEGR  & (begin, left)          & $V_{\mbox{\tiny BEGR}} \rightarrow Y$       \\
END   & (end)                  & $V_{\mbox{\tiny END}}  \rightarrow \epsilon$ \\ \hline
\end{tabular}
 
These node types correspond closely with a CM's final state types.
The guide tree deals with the consensus structure; for individual
sequences we will need to deal with insertions and deletions with
respect to this consensus. The guide tree is the skeleton on which we
will later organize the CM.

The input alignment is first used to construct a consensus secondary
structure (Figure~\ref{fig:cm_nodetree}, left) that defines which
aligned columns will be ignored as non-consensus (and later modeled as
insertions relative to the consensus), and which consensus alignment
columns are base-paired to each other.

Given the consensus structure, consensus base pairs are assigned to
MATP nodes and consensus unpaired columns are assigned to MATL or MATR
nodes. One ROOT node is used at the head of the tree.  Multifurcation
loops and/or multiple stems are dealt with by assigning one or more
BIF nodes that branch to subtrees starting with BEGL or BEGR head
nodes. (ROOT, BEGL, and BEGR start nodes are labeled differently
because they will be expanded to different groups of states; this has
to do with avoiding ambiguous parse trees for individual sequences, as
will soon become apparent.) Alignment columns that are considered to
be insertions relative to the consensus structure are ignored at this
stage.

There will in general be more than one possible guide tree for any
given consensus structure. Almost all of this ambiguity is eliminated
by three conventions: (1) only MATL nodes are used for hairpin loops;
(2) in describing interior loops, MATL nodes are used before MATR
nodes; and (3) BIF nodes are only invoked where necessary to explain
branching secondary structure stems (as opposed to unnecessarily
bifurcating in single stranded sequence). One source of ambiguity
remains. In invoking a bifurcation to explain alignment columns $i..j$
by two substructures on columns $i..k$ and $k+1..j$, there may be
several possible choices of $k$ (e.g. in a single stranded region
between the two substructures). The choice of bifurcation point $k$
impacts the performance of the divide and conquer algorithm.  My
current strategy is to choose $k$ such that $i..k$ and $k+1..j$ are as
close to the same length as possible.

The result of this procedure is the guide tree. The nodes of the guide
tree are numbered in post-traversal order (from root to leaves, so
parent nodes always have lower indices). The guide tree corresponding
to the input multiple alignment in Figure~\ref{fig:input_alignment} is
shown on the right in Figure~\ref{fig:cm_nodetree}.

\subsubsection{From guide tree to covariance model}

A CM must deal with insertions and deletions in individual sequences
relative to the consensus structure. For example, for a consensus base
pair, either partner may be deleted leaving a single unpaired residue,
or the pair may be entirely deleted; additionally, there may be
inserted nonconsensus residues between this pair and the next pair in
the stem. Accordingly, each node in the master tree is expanded into
one or more \emph{states} in the CM as follows:

\begin{tabular}{clccc}
Node   &  States             & nstates & nsplit & ninsert \\ \hline
MATP   & [MP ML MR D] IL IR  &   6     &   4    &  2   \\
MATL   & [ML D] IL           &   3     &   2    &  1   \\
MATR   & [MR D] IR           &   3     &   2    &  1   \\
BIF    & [B]                 &   1     &   1    &  0   \\
ROOT   & [S] IL IR           &   3     &   1    &  2   \\
BEGL   & [S] IL              &   1     &   1    &  0   \\
BEGR   & [S] IL IR           &   2     &   1    &  1   \\
END    & [E]                 &   1     &   1    &  0   \\ \hline
\end{tabular}

The states are grouped into a ``split set'' of 1-4 (shown in brackets
above) and an ``insert set'' of 0-2 insert states. The split set
includes the consensus state, which by convention is first. One and
only one of the states in the split set must be visited in every parse
tree (and this fact will be exploited by the divide and conquer
algorithm). The insert state(s) are not obligately visited, and they
have self-transitions, so they will be visited zero or more times in
any given parse tree.

State transitions are then assigned as follows. For bifurcation nodes,
the B state makes transitions to the S states of the child BEGL and
BEGR nodes. Each state in a split set of a non-bifurcation node makes
a possible transition to every insert state in the \emph{same} node,
and to every state in the split set of the \emph{next} node. An IL
state makes a transition to itself, to the IR state in the same node
(if present), and to every state in the split set of the next node. An
IR state makes a transition to itself and to every state in the split
set of the next node.

This arrangement of transitions guarantees that there is unambiguously
one and only one parse tree for any given individual structure. This
is important, because the algorithm will find a maximum likelihood
parse tree for a given sequence, and we wish to interpret this result
as a maximum likelihood structure.

As a convenient side effect of the ordering of the states, it is
guaranteed that the transitions from any state are to a
\emph{contiguous} set of child states, so the transitions for state
$v$ may be kept as an offset and a count; e.g. state $v$ makes
transitions to states $v+\mbox{offset}_v..v+{count}_v-1$, with offset
always $\geq 0$.  Similarly, the parent states which may transition
into $v$ are also guaranteed to be contiguous in the final CM. This
contiguity is exploited by the implementation.

Thus the final CM structure is an array of M states, connected as a
directed graph by transitions $t_v(y)$ (or probability 1 transitions
$v \rightarrow y,z$ for bifurcations) with the states numbered such
that $y,z \geq v$. The numbering of the states is in postorder
traversal with respect to the guide tree. There are no cycles in the
directed graph other than cycles of zero length (e.g. the
self-transitions of the insert states). An example CM, corresponding
to the input alignment of Figure~\ref{fig:input_alignment}, is shown
in Figure~\ref{fig:cm_graph}.

\subsubsection{Parameterization}

Using the guide tree and the final CM, each individual sequence in the
input multiple alignment can be converted unambiguously to a CM parse
tree, as shown in Figure~\ref{fig:parsetrees}. Counts for observed
state transitions and singlet/pair emissions are then collected from
these parse trees. The observed counts are converted to transition and
emission probabilities by standard procedures (I calculate maximum a
posteriori parameters, using Dirichlet priors).

\subsubsection{Comparison to profile HMMs}

The relationship between an SCFG and a covariance model is analogous
to the relationship of hidden Markov models (HMMs) and profile HMMs
for modeling multiple sequence alignments
\cite{Krogh94,Durbin98,Eddy98}.  The contrast may be instructive, at
least to readers familiar with profile HMMs.  A profile HMM is a
repetitive HMM architecture that associates each consensus column of a
multiple alignment with a single type of model node -- a MATL node, in
the above notation. Each node contains a ``match'', ``delete'', and
``insert'' HMM state -- ML, IL, and D states, in the above notation.
The profile HMM also has special begin and end states. Profile HMMs
are therefore essentially a special case of CMs. An unstructured RNA
multiple alignment would be modeled by a guide tree of all MATL nodes,
and converted to an unbifurcated CM that would essentially be
identical to a profile HMM. (The only difference is trivial; the CM
root node includes a IR state, whereas the start node of a profile HMM
does not.) All the other node types (especially MATP, MATR, and BIF)
and state types (e.g. MP, MR, IR, and B) are SCFG augmentations
necessary to extend profile HMMs to deal with RNA secondary structure.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Divide and conquer algorithm}

I use $v$, $w$, $y$, and $z$ as indices of states in the model. These
will range from $1..M$, for a CM with $M$ states. $S_v$ refers to the
\emph{type} of state $v$; it will be one of the nine types
\{D,P,ML,IL,MR,IL,S,E,B\}. $C_v$ is a list of children for state $v$
(states $y$ that $v$ can transit to); it will contain up to six
contiguous indices $y$ with $v <= y <= M$. $P_v$ is a list of parents
for state $v$ (states that could have transited to state $v$); it will
contain up to six contiguous indices $y$ with $1 <= y <= v$. (Because
the sets are guaranteed to be contiguous in the model, the
implementation stores these as an offset relative to $v$ and a count,
as described previously.) I use $g$, $h$, $i$, $j$, $k$, $p$, and $q$
as indices referring to positions in a sequence $x$. These indices
will range from $1..L$, for a sequence of length $L$.

The Inside/CYK algorithm calculates $\alpha_v(i,j)$ -- the log
probability of the best alignment of a CM parse subtree rooted at
state $v$ generating every subsequence $x_i..x_j$ of sequence $x$.
The Outside/CYK algorithm calculates $\beta_v(i,j)$ -- the log
probability of the best alignment of the CM and sequence $x$
\emph{excluding} the best alignment for the CM subgraph rooted at $v$
generating the subsequence $x_i..x_j$.  For any chosen cell in the 3D
dynamic programming matrices with coordinates $v,i,j$, $\alpha_v(i,j)
+ \beta_v(i,j)$ gives us the score of the best alignment that passes
through that cell. 

Thus, for any chosen state $v$, $\argmax_{i,j} \alpha_{v}(i,j) +
\beta_{v}(i,j)$ tells us which cell $v,i,j$ the optimal parse tree
passes through, \emph{conditional on using state $v$ in the parse}.

We also know that any parse tree must include all the bifurcation and
start states of the CM, so we know that the optimal alignment
\emph{must} use any chosen bifurcation state $v$ and its child start
states $w$ and $y$. One of the hearts of the divide and conquer
algorithm is that we are guaranteed that:

\[
   \max_{i,k,j} \beta_{v}(i,j) + \alpha_{w}(i,k) + \alpha_{y}(k+1,j)
\]

is the optimal overall alignment score, and we also know that

\[
      (i,k,j) = \argmax_{i',k',j'}  \beta_{v}(i',j') +
      \alpha_{w}(i',k') + \alpha_{y}(k'+1,j') 
\]

gives us a triplet that identifies three cells that must be in the
optimal alignment -- $(v,i,j)$, $(w,i,k)$, and $(y,k+1,j)$. This
splits the remaining problem into three smaller subproblems -- an
alignment of the sequence $x_{i}..x_{k}$ to a CM subgraph $w..y-1$, an
alignment of the sequence $x_{k+1}..x_{j}$ to a CM subgraph $y..M$,
and an alignment of the two-piece sequence
$x_1..x_{i-1}//x_{j+1}..x_L$ to a CM subgraph $1..v$.

The subproblems are then themselves split, and this splitting can
continue recursively until all the optimal bifurcation points on the
optimal parsetree have been determined.

At this point the remaining alignment subproblems might be small
enough to be solved by straightforward application of the standard
Inside/CYK algorithm . However, this is not guaranteed to be the
case. A second division strategy is needed that does not depend on
splitting at bifurcations.

For the second strategy, we take advantage of the fact that we know
that the optimal parse tree must also include one and only one state
from the split set of each node (e.g. the non-insert states in the
node). Let $N(\bar{v})$ represent a list of the indices for the split
set in the node that includes state $\bar{v}$ (where it is possible
that $\bar{v}$ is not a member of $N(\bar{v})$, if $\bar{v}$ is an
insert state). ($N(\bar{v})$ contains at most 4 states.) Then for any
alignment subproblem we can choose a provisional split point $\bar{v}$
in the midpoint of the model graph. We know that

\[
(v,i,j) = \argmax_{v' \in N(\hat{v}),i',j'} \alpha_{v'}(i',j') + \beta_{v'}(i',j')
\]

gives us a new cell $(v,i,j)$ in the optimal parse tree, and splits
the problem into two smaller problems. This strategy can be applied
recursively all the way down to single nodes. We can therefore
guarantee that we will never need to carry out a full Inside/CYK
alignment algorithm on any subproblem. The most memory-intensive
alignment problem that needs to be solved is the very first split.
The properties of the first split determine the memory complexity of
the algorithm.

Therefore we will have to deal with three types of problems
(Figure~\ref{fig:splitter_schematic}):

\begin{itemize}
\item A \emph{generic problem} means finding the optimal alignment of
a CM subgraph $G_{r..z}$ to a contiguous subsequence $x_g..x_q$. The
subgraph $G_{r..z}$ corresponds to a complete subtree of the CM's
guide tree -- e.g. state $r$ is a start (S), and state $z$ is an end
(E). $G_{r..z}$ may contain bifurcations. The problem is solved in one
of two ways. If $G_{r..z}$ contains no bifurcations, it is solved as a
wedge problem (see below). Else, the problem is subdivided by the
bifurcation-dependent strategy; an optimal triple $(i,k,j)$ is found
for a bifurcation state $v$ and its children $w,y$, splitting the
problem into a V problem and two generic problems.

\item A \emph{wedge problem} means finding the optimal alignment of an
unbifurcated CM subgraph $G_{r..z}$ to a contiguous subsequence
$x_g..x_q$. State $r$ does not have to be a start state (S); it may be
a state in a split set (MP, ML, MR, or D). State $z$ is an end (E).  A
wedge problem is solved by the split set-dependent strategy: an
optimal $(v,i,j)$ is found, splitting the problem into a V
problem and a smaller wedge problem.

\item A \emph{V problem} consists of finding the optimal alignment of
an unbifurcated CM subgraph $G_{r..z}$ to a noncontiguous, two-piece
sequence $x_g..x_h//x_p..x_q$, exclusive of the residues $x_h$ and
$x_p$ (open circles in Figure~\ref{fig:splitter_schematic}).  State
$r$ can be a start state or any state in a split set; the same is true
for $z$. A V problem is solved by a split set-dependent strategy: an
optimal $(v,i,j)$ is found, splitting the problem into two V problems.
\end{itemize}

\subsection{generic\_splitter}

\begin{algorithm}
\alginout{A generic problem, for CM subgraph $G_{r..z}$ and
          subsequence $x_{g..q}$.}
         {An optimal parse subtree ${\cal T}^{r..z}_{g..q}$.}

\algname{generic\_splitter}{r,z; g,q}
\begin{algtab}
\algif{alignment problem is small:}
  \algreturn \algcall{inside$^\mathcal{T}$}{r,z; g,q}\\
\algelseif{no bifurcation in $G_{r..z}$:}
  \algreturn \algcall{wedge\_splitter}{r,z; g,q}\\
\algelse
   $v   \leftarrow$ lowest numbered bifurcation state in subgraph $G_{r..z}$.\\
   $w,y \leftarrow$ left and right S children of $v$.\\
   \\
   $\beta_v \leftarrow$  \algcall{outside}{r,w; g,q}\\
   $\alpha_w \leftarrow$ \algcall{inside}{w,y-1; g,q}\\
   $\alpha_y \leftarrow$ \algcall{inside}{y,z; g,q}\\

   $(i,k,j) \leftarrow \argmax_{i',k',j'} \alpha_w(i',k') + \alpha_y(k'+1,j') + \beta_v(i',j')$ \\

   $\mathcal{T}_1 \leftarrow$ \algcall{V\_splitter}{r,v; g,i; j,q}\\
   $\mathcal{T}_2 \leftarrow$ \algcall{generic\_splitter}{w,y-1; i,k}\\
   $\mathcal{T}_3 \leftarrow$ \algcall{generic\_splitter}{y,z; k+1,j}\\

   \algreturn $\mathcal{T}_1 (\mathcal{T}_2, \mathcal{T}_3)$\\
\algend
\end{algtab}
\end{algorithm}

\subsection{wedge\_splitter}

\begin{algorithm}
\alginout{A wedge problem, for unbifurcated CM subgraph $G_{r..z}$ and
          subsequence $x_{g..q}$.}
         {An optimal parse subtree ${\cal T}^{r..z}_{g..q}$.}

\algname{wedge\_splitter}{r,z; g,q}
\begin{algtab}
\algif{alignment problem is small:}
  \algreturn \algcall{inside$^\mathcal{T}$}{r,z; g,q}\\
\algelse
  $(w..y) \leftarrow$ a split set chosen from middle of $G_{r..z}$\\
  $(\alpha_w..\alpha_y) \leftarrow$ \algcall{inside}{w,z; g,q}\\
  $(\beta_w..\beta_y)   \leftarrow$ \algcall{outside}{r,y; g,q}\\
  $(v,i,j) \leftarrow \argmax_{v'=w..y, j'=1..L, i'=1..j'} 
	\alpha_{v'}(i',j') + \beta_{v'}(i',j')$\\
  $\mathcal{T}_1 \leftarrow$ \algcall{V\_splitter}{r,v; g,i; j,q}\\
  $\mathcal{T}_2 \leftarrow$ \algcall{wedge\_splitter}{v,z; i,j}\\
  \algreturn $\mathcal{T}_1 (\mathcal{T}_2)$\\
\algend
\end{algtab}
\end{algorithm}

\subsection{V\_splitter}

The V\_splitter subroutine is given a specialized V type alignment
subproblem, defined by an unbifurcated, unterminated CM segment $G$
and two subsequences $a_1..a_L//b_1..b_N$ of the original sequence. It
solves the problem and returns the optimal parse tree $\cal{T}$. The
solution may be achieved recursively by splitting the V problem into
two smaller V problems and solving them.

Determine the size of the alignment problem. If it is small enough,
obtain $\cal{T}$ by calling v\_inside$^{\cal{T}}(G, a_1..a_L//b_1..b_N)$;
terminate.

Let $w..y$ be a chosen ``split set'' in $G$: all the non-insert states
from a node in the middle of $G$, as in wedge\_splitter.

Call v\_inside$(G^w, a_1..a_L//b_1..b_N)$ to obtain decks $\alpha_w..\alpha_y$.
Call v\_outside$(G^0_y, a_1..a_L//b_1..b_N)$ to obtain decks $\beta_w..\beta_y$.

Find $(v,i,j) = \argmax_{v'=w..j, i'=1..L, j'=1..N} \alpha_{v'}(i',j') + \beta_{v'}(i',j')$

$\cal{T}_1 = \mbox{V\_splitter}(G^1_v, a_1..a_i, b_j..b_N)$
$\cal{T}_2 = \mbox{V\_splitter}(G^v_w, a_i..a_L, b_1..b_j)$

$\cal{T} = \cal{T}_1 (\cal{T}_2)$; terminate.

\subsection{inside and inside$^{\cal{T}}$}

The subroutine inside implements the Inside/CYK SCFG alignment
algorithm in a form that minimizes the memory requirement by
discarding most of the 3D matrix during the calculation.

The algorithm works by iteratively calculating $\alpha_v(i,j)$, the
log probability of the most likely parse tree that generates a
subsequence $a_i..a_j$ from a subgraph of the model rooted at state
$v$. The calculation initializes with the smallest subgraphs and
subsequences (subgraphs rooted at end states generating subsequences
of length 0). It iterates over longer and longer subsequences and
larger and larger subgraphs until it has calculated $\alpha_0(1,L)$ --
the log probability of the best alignment of the whole CM $G$ to the
whole sequence $a_1..a_L$.

\begin{verbatim}
Recursion:
for v = M to 1:
    Allocate memory for a new deck \alpha_v with rows j=0..L, columns i=1..j+1.
    Allocate a shadow deck \tau_v.
    Set all values in \alpha_v to -\infty, and \tau_v to -1.

    for j = 0 to L; for i = j+1 to 1:
     d = j-i+1;
     
     for S_v: &  \alpha_v(i,j) =                                                    & \tau(i,j) = \\
     D,S:     &\max_{y \in C(v)} \alpha_y(i,j)  + \log t_v(y)	                    & \argmax_y\\
     P,d>=2:  &\log e(a_i, a_j) + \max_{y \in C(v)} \alpha_y(i+1,j-1) + \log t_v(y) & \argmax_y\\
     L,d>=1:  &\log e(a_i) +      \max_{y \in C(v)} \alpha_y(i+1,j)   + \log t_v(y) & \argmax_y\\
     R,d>=1:  &\log e(a_j) +      \max_{y \in C(v)} \alpha_y(i,j-1)   + \log t_v(y) & \argmax_y\\
     B:       &\max_k \alpha_y(i,k) + \alpha_z(k+1,j)                               & \argmax_k\\
     E,d=0:   & 0                                                                   & -1\\

    for each y \in C(v): if all P(y) > v, free the memory for deck \alpha_y.
\end{verbatim}

The $\tau$ values are a ``shadow matrix'' that requires further
explanation. In some cases, we will call inside as inside$^{\cal{T}}$.
In these cases, we know the problem is small enough that a divide and
conquer is no longer needed, and we want to trace back an optimal
parse tree $\cal{T}$. One way to do this would be to keep a full
$\alpha$ matrix in memory and tracing back by recapitulating the
calculations in reverse. It is easier and more memory efficient to
keep a ``shadow'' traceback pointer matrix $\tau$. (The memory
efficiency results from the fact that the non-bifurcation $\tau$'s
only need a range of 0..5 and fit in a few bits, compared to
$\alpha$'s which are log probabilities requiring greater dynamic range
and more bits.)  A $\tau_v(i,j)$ traceback pointer either records the
index $y$ that maximized $\alpha_v(i,j)$ (for state types D,S,P,L,R)
or records the split point $k$ that maximized $\alpha_v(i,j)$ for a
bifurcation (B) state. We trace back through the shadow matrix to
obtain the optimal sub-alignment that gave the optimal score. The most
simple explanation of this traceback is a recursive one: we call
subroutine traceback(v=0, i=1, j=L):

\begin{verbatim}
subroutine traceback(v,i,j):
  S_v = E:     attach v;
  S_v = S,D :  attach v;           traceback(\tau_v(i,j), i, j).
  S_v = P :    attach a_i,v,a_j;   traceback(\tau_v(i,j), i+1, j-1).
  S_v = L :    attach a_i,v;       traceback(\tau_v(j,d), i+1, j).
  S_v = R :    attach v,a_j;       traceback(\tau_v(j,d), i,   j-1).
  S_v = B :    attach v;           traceback(y, i, \tau_v(j,d));   traceback(z, \tau_v(i,j)+1, j).
\end{verbatim}

When inside is called (as opposed to inside$^{\cal{T}}$), none of the
$\tau$ shadow operations are performed in inside. Instead, the 3D
scoring matrix itself is returned, containing the deck $\alpha_v$ (and
possibly a few other valid, unfree'd score decks; see below).

The memory handling in inside\_engine also needs further explanation.
Recall that the key to the memory efficiency of the divide and conquer
approach is that we don't need to keep the entire $\alpha$ matrix in
memory to calculate scores. In the above formulation, with the outer
loop ranging over decks for the CM states $v$, as we reach higher
decks $v$ in the 3D matrix, our calculations may no longer depend on
certain lower decks. Since we are moving sequentially ``upwards'' in
the states from state $M$ to state $1$, a simple test (all P(y) >
v?)  is sufficient to determine whether the scores in deck y will be
needed by any higher decks; if deck y is not needed, its memory can be
released. (Since all the decks are the same size, an implementation
can recycle ``free'd'' decks, saving memory allocation calls. Also,
note that the values in all end decks are identical, so only one end
deck needs to be calculated, and that precalculated deck can be reused
whenever $S_v = E$.)

This rule, combined with the structure of a CM, has an additional nice
property. When inside is called by generic\_splitter, for a CM graph
$G$ in which state 1 is always a start state, we only need the deck
$\alpha_1$ returned -- and since all the states $y$ that a start state
connects to cannot have another parent higher in the CM, when the
algorithm completes, $\alpha_1$ is the only deck left unfree'd.
[more here]





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{outside}

The subroutine outside\_engine implements a variant of the SCFG
Outside/CYK algorithm. Outside/CYK calculates $\beta_v(j,d)$, the log
probability of the optimal parse tree for a CM generating a sequence
$x_1..x_L$ \emph{excluding} the optimal parse subtree rooted at state
$v$ that accounts for the subsequence of length $d$ that ends at $j$.
The outside\_engine has three variations from the standard algorithm.
Two are analogous to the inside\_engine. First, outside\_engine can be
called not just for the complete model and sequence, but for any CM
subgraph of $\hat{M}$ states rooted at $\hat{r}$ and any subsequence
$x_{\hat{i}}..x_{\hat{j}}$. Second, it does not keep the entire $\beta$
matrix in memory -- it returns only the $\beta_{\hat{v}}$ deck. Third,
outside\_engine will only be called on a linear ``segment''
$r..\hat{v}$ of the CM that starts in a start state ($S_r = S$), ends
in a bifurcation state ($S_{\hat{v}} = B$), and has no other start or
bifurcation states, so the recursion is simpler than the standard
Outside/CYK algorithm.

\begin{verbatim}
Initialization:
Obtain a deck $\beta_r$ for the root state $r$.
Set $\beta_{r}(\hat{j}, \hat{L}) = 0$.
Set all other $\beta_r$ values to $-\infty$.

Recursion:
for v = r+1 to \hat{v}
  Obtain a deck for $\beta_v$ (rows $j = \hat{i}-1..\hat{j}$, columns $d = 0..j-\hat{i}+1$
  
  for j = \hat{j} to \hat{i}-1
    for d = j-\hat{i}+1 to 0
       
       \beta_v(j,d) = \max_{y \in P(v)} \left{ 
              S_y = D,S,E: \beta_y(j,d)     + \log t_y(v) 
              S_y = P:     \beta_y(j+1,d+2) + \log t_y(v) + \log e_y(x_{i-1}, x_{j+1})
              S_y = L:     \beta_y(j,d+1)   + \log t_y(v) + \log e_y(x_{i-1})
              S_y = R:     \beta_y(j+1,d+1) + \log t_y(v) + \log e_y(x_{j+1})
                    (B impossible)

         boundary conditions:
              S_y = P: for j = \hat{j} or d = j-\hat{i}+1, assume \beta_y(j+1,d+2) = -\infty
	      S_y = L: d = j-\hat{i}+1, assume \beta_y(j,d+1) = -\infty
              S_y = R: j = \hat{j},     assume \beta_y(j+1,d+1) = -\infty

  for each y \in P(v): if all C(y) < v, release the deck \beta_y.
\end{verbatim}

At the end of the calculation, outside\_engine returns the $\beta_v$
deck.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{segment\_alignment\_engine}


\subsubsection{Implementation}

Full source code for a reference implementation is freely available at
XXX under a GNU General Public License. This is an ANSI C codebase
that has been tested on GNU/Linux platforms.

The CM data structure is defined in \texttt{structs.h}. The CM
construction procedure is in
\texttt{modelmaker.c:Handmodelmaker()}. The guide tree is constructed
in \texttt{HandModelmaker()}. A CM is constructed from the guide tree
by \texttt{cm\_from\_master()}. Individual parse trees are constructed
using the guide tree by \texttt{transmogrify()}.

The divide and conquer algorithm is implemented in
\texttt{smallcyk.c:CYKDivideAndConquer()}, which will recursively call
a set of functions: the three splitting routines
\texttt{generic\_splitter()}, \texttt{wedge\_splitter()}, and
\texttt{V\_splitter}; the four alignment engines \texttt{inside()},
\texttt{outside()}, \texttt{vinside()}, and \texttt{voutside()}; and
the two traceback routines \texttt{insideT()} and \texttt{vinsideT()}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{distilled}
\end{document}



