\documentclass[11pt]{article}
\usepackage{times}
\newif\ifpdf
\ifx\pdfoutput\undefined
  \pdffalse
  \usepackage[dvips]{graphicx}    
  \usepackage[dvips]{color}  
\else
  \pdftrue
  \usepackage[pdftex]{graphicx}    
  \usepackage[pdftex]{color}  
\fi
\usepackage{fullpage}
\usepackage{url}
\usepackage{cite}
\usepackage{alg}
\bibliographystyle{bmc_bioinformatics}

\def\argmax{\mathop{\mathrm{argmax}}\limits}
\def\argmin{\mathop{\mathrm{argmin}}\limits}

\setcounter{secnumdepth}{0}

\begin{document}

\title{A memory-efficient dynamic programming algorithm for optimal
alignment of a sequence to an RNA secondary structure consensus model}
\author{Sean R. Eddy\\
Howard Hughes Medical Institute \& Department of Genetics, \\
Washington University School of Medicine \\
Saint Louis, Missouri 63110 USA \\
eddy@genetics.wustl.edu}  
\date{\today}
\maketitle

\abstract{The problem of optimally aligning an RNA sequence to an RNA
secondary structure consensus model is considered. Stochastic
context-free grammars (SCFGs) provide a probabilistic framework for
modeling both sequence conservation and long-distance base pairing
correlations of an RNA secondary structure consensus. Covariance
models (CMs) are a restricted case of SCFGs specifically suited to
modeling RNA structure consensus, analogous to profile hidden Markov
models for linear sequence consensus. The standard CYK algorithm for
optimal dynamic programming alignment of a CM/SCFG to an RNA sequence
is $\Theta(N^3)$ in memory for an RNA of length $N$. This is only
practical for small RNAs of a few hundred nucleotides or less. Here I
describe a divide and conquer variant of the CM/SCFG alignment
algorithm, analogous to the memory-efficient Myers/Miller form of the
Smith/Waterman algorithm for linear sequence alignment. The algorithm
has an average memory requirement proportional to $(N^2 \log N)$, at
the expense of a small constant factor in time. Small subunit
ribosomal RNA alignments that previously required a prohibitive 20-30
GB of memory to solve by dynamic programming now require $\approx$ 80
MB and are well within the capabilities of most modern computers.}

\section{Introduction}

Given a structural RNA of known secondary structure, I want to
identify homologous structural RNAs in a sequence database. Because
RNAs often tend to conserve a base-paired secondary structure more
than they conserve primary sequence, it would be best if database
searches for homologous RNAs took into account both the primary and
secondary structure of the query RNA.

Some excellent approaches already exist in the literature. For
example, exact- and approximate-match pattern searches (a la PROSITE
patterns) can and have been extended to allow patterns to specify
long-range base pairing constraints. Three such programs are PATSCAN,
RNAMOT, and RNABOB. Another approach involves developing a specialized
program to recognize a specific type of RNA -- for example, several
programs exist for detecting transfer RNA genes, one for group I
catalytic introns, and several groups, including mine, have recently
used computational searches for new small nucleolar RNAs.

In primary sequence analysis, a technologically more mature field, the
main techniques are neither pattern searches nor specialized
programs. The main techniques are sequence alignment algorithms with
probabilistically-based scoring systems -- for example, the BLAST or
FASTA algorithms, and the PAM or BLOSUM score matrices. Unlike
specialized programs, a general alignment algorithm can be applied to
any query sequence. Unlike pattern searches, which give yes/no answers
for whether a candidate sequence is a match, a scoring system gives a
meaningful score that allows ranking candidate hits by their
statistical significance. It is thus of interest to develop general
alignment algorithms for structural RNA queries.

Eddy and Durbin and Sakakibara \emph{et al.} introduced stochastic
context free grammar (SCFG) algorithms as a general approach to RNA
alignment \cite{Eddy94,Sakakibara94c,Durbin98}. SCFGs allow the strong
pairwise residue correlations in non-pseudoknotted RNA secondary
structure to be taken into account in pairwise or profile RNA
alignments, while using a dynamic programming algorithm that
guarantees finding a mathematically optimal alignment in polynomial
time. SCFG alignment algorithms are a natural extension of standard
linear sequence alignment algorithms (particularly those with fully
probabilistic, hidden Markov model formulations) into an additional
dimension necessary to deal with 2D RNA secondary structure.

While SCFGs may provide a natural mathematical framework for RNA
secondary structure alignment problems \cite{Durbin98}, SCFG
algorithms have high computational complexity. Optimal SCFG-based
structural alignment of an RNA structure to a sequence costs $O(N^3)$
memory and $O(N^4)$ time for a sequence of length $N$, compared to
$O(N^2)$ memory and time for sequence alignment algorithms. Corpet and
Michot have described RNAlign, a program that also implements a
different general dynamic programming algorithm for RNA alignment; the
RNAlign algorithm solves the same problem as SCFGs even less
efficiently, requiring $O(N^5)$ time and $O(N^4)$ memory.  SCFG-based
alignments of small structural RNAs are feasible. Using my COVE
software, transfer RNA alignments (~75 nucleotides) take about 1 cpu
second and 3 Mb of memory. Most genome centers now use an SCFG-based
search program, tRNAscan-SE, for annotating transfer RNA genes
\cite{LoweEddy97}. However, many larger RNAs of interest are outside
the capabilities of published SCFG alignment algorithms. Alignment of
a small subunit (SSU) ribosomal RNA sequence to the SSU rRNA consensus
structure would take about 23 Gb of RAM and 8 hours of CPU time. The
steep memory requirement is particularly galling. It constitutes a
significant practical barrier to the wider applicability of SCFG
algorithms.

% The above figures come from INFERNAL with manually set L.
% cmbuild --rf foo.cm tRNA1415.sto:
%    M=233 L=75 
%    Full CYK = 2.7 Mb. 822206 cycles.  ~ 1 sec.
%    Small = 0.14 Mb.
% cmbuild foo.cm ssu.sto
%    M=4789 L=1542
%    Full CYK = 22.7 Gb. 2.4e10 cycles. ~ 8 hr.
%    Small = 86 Mb.

Notredame \emph{et al.} have pointed specifically at this problem.
They described RAGA \cite{Notredame97}, a program that uses a genetic
algorithm (GA) to optimize a pairwise RNA alignment using an objective
function that includes base pairing terms. Because GAs have an $O(N)$
memory requirement, RAGA can effectively find reasonable solutions for
large RNA alignment problems (such as ribosomal RNA
alignments). Although RAGA does not guarantee a mathematically optimal
solution, it is probably the current state of the art for structural
alignment of large RNAs.

Here, I introduce a dynamic programming solution to the problem of
structural alignment of large RNAs. The central idea is a ``divide and
conquer'' strategy. There is a historical parallel.  For linear
sequence alignment, a divide and conquer algorithm was introduced by
Hirschberg \cite{Hirschberg75}, an algorithm better known in the
computational biology community as the Myers/Miller algorithm
\cite{MyM-88a}.  Ironically, at the time, dynamic programming methods
for optimal sequence alignment were well known, but were considered
impractical on 1970's era computers because of an ``extreme'' $O(N^2)$
memory requirement. Myers/Miller reduces the memory complexity of a
dynamic programming sequence alignment algorithm from $O(N^2)$ to
$O(N)$, at the cost of a constant (roughly two-fold) increase in CPU
time. Here I show that a divide and conquer strategy can also be
applied to a three dimensional SCFG dynamic programming matrix,
greatly reducing the memory requirement of SCFG alignments and making
optimal structural alignment of large RNAs possible on our current
generation of computers.

I will strictly be dealing with the problem of aligning a target
sequence of unknown structure to a query RNA of known secondary
structure. By ``secondary structure'' I mean nested (nonpseudoknotted)
pairwise RNA secondary structure interactions, primarily Watson-Crick
but also allowing other noncanonical RNA base pairs. I refer to this
as an RNA structural alignment problem to distinguish it from linear
sequence alignment. I am not addressing the problem of structurally
superposing molecules of known three-dimensional structure, nor the
problem of structurally aligning known RNA secondary structures to
each other, nor the problem of trying to simultaneously infer both a
consensus secondary structure and a secondary structure alignment for
a set of unaligned RNA sequences of unknown structure.

\section{Algorithm}

\subsection{Prelude: the simpler case of sequence alignment}

The essential concepts of a divide and conquer alignment algorithm are
most easily understood for the case of primary sequence alignment
\cite{Hirschberg75,MyM-88a}. 

Dynamic programming (DP) algorithms for sequence alignment iteratively
fill in an $N \times M$ DP matrix of scores $F(i,j)$
\cite{Needleman70,Smith81}. Each score $F(i,j)$ is the score of the
optimal alignment of prefix $x_1..x_i$ of one sequence to prefix
$y_1..y_j$ of the other. These scores are calculated iteratively, e.g.
for global (Needleman/Wunsch) alignment:

\[  F(i,j) = \max \left\{ \begin{array}{l}
                       F(i-1,j-1) + \mbox{score for aligning} x_i, y_j \\
                       F(i-1,j) - \mbox{cost of a gap character} \\
                       F(i,j-1) - \mbox{cost of a gap character} \\
                       \end{array} \right. \]

At the end, $F(N,M)$ contains the score of the optimal alignment. The
alignment itself is recovered by tracing the individual optimal steps
backwards through the matrix, starting from cell $(N,M)$. The
algorithm is $O(NM)$ in both time and memory.

If we are only interested in the score, not the alignment itself, the
whole $F$ matrix does not have to be kept in memory. The iterative
calculation only depends on the current and previous row of the
matrix. Keeping two rows in memory suffices, so a score-only
calculation can be done in $O(N)$ space.

The fill stage of DP alignment algorithms may be run either forwards
and backwards. We can just as easily calculate the optimal score
$B(i,j)$ of the best alignment of the \emph{suffix} $i+1..N$ of
sequence 1 to the suffix $j+1..M$ of sequence 2, until one obtains
$B(0,0)$, the overall optimal score -- the same number as $F(N,M)$.

The sum of $F(i,j)$ and $B(i,j)$ at any cell in the optimal path
through the DP matrix is also the optimal overall alignment score.
More generally, $F(i,j) + B(i,j)$ at any cell $(i,j)$ is the score of
the best alignment that uses that cell. Therefore, since we know the
optimal alignment must pass through any row $i$ \emph{somewhere}, we
can pick some row $i$ in the sequence, run the forward calculation to
$i$ to obtain row $F(i)$, run the backwards calculation back to $i$ to
get row $B(i)$, and then find $\argmax\nolimits_j F(i,j) + B(i,j)$, which tells
me that the optimal alignment passes through cell $(i,j)$. (For
clarity, I am leaving out details of how indels and local alignments
are handled.)

This divides the alignment into two smaller alignment problems. These
smaller problems can themselves be subdivided by the same trick.
Thus, the complete optimal alignment can be found by a recursive
series of split point calculations. Although this seems laborious --
each calculation gives us only a single point in the alignment -- if
we choose our split row $i$ to be in the middle, the size of the two
smaller DP problems is decreased by about 4-fold at each split. A
complete alignment costs only about $1 + \frac{2}{4} + \frac{4}{16} +
\ldots \simeq 2$ times as much CPU time as doing the alignment in a
single DP matrix calculation, but the recursive divide-and-conquer
approach costs only $O(N)$ memory.

For SCFG alignment, the standard Cocke-Younger-Kasami (referred to in
this paper as Inside/CYK) DP alignment algorithm can also be run in a
memory-saving ``score only'' mode, as can the backwards analogue,
Outside/CYK. However, the Inside/CYK and Outside/CYK algorithms are
not as nicely symmetrical as the forward and backward sequence
alignment algorithms, and the splitting procedure that one obtains
does not generate identical types of subproblems, so the divide and
conquer procedure for SCFG-based RNA alignment is not as obvious.

Rather than showing generalized memory-efficient SCFG alignment
algorithms, I will describe algorithms specific for ``RNA covariance
models'' (CMs), a specialized form of SCFG specifically designed for
RNA secondary structure alignment. My algorithm takes advantage of
features of CMs that do not apply to SCFGs in general. I will start
with an introduction to what CMs are, how they correspond to a known
RNA secondary structure, and how they are built and parameterized.
      
\subsection{Definition and construction of a covariance model}

A covariance model is a restricted form of stochastic context free
grammar that is well suited for modeling a consensus RNA secondary
structure.

\subsubsection{Definition of a stochastic context free grammar}

A stochastic context free grammar (SCFG) consists of the following:

\begin{itemize}
\item $M$ nonterminals (here called \emph{states}).
\item $K$ terminal symbols (e.g. the observable alphabet, {A,C,G,U} for RNA)
\item a number of \emph{production rules} of the form:
      $V \rightarrow \alpha$,
      where $\alpha$ can be any string of nonterminal and/or terminal
      symbols, including the empty string
      $\epsilon$.
\item Each production rule is associated with a probability, such that
      the sum of the production probabilities for any given
      nonterminal $V$ is equal to 1.
\end{itemize} 

\subsubsection{Relationship between SCFGs and covariance models}

A covariance model is a specific repetitive SCFG architecture
consisting of groups of model states that are associated with base
pairs and single-stranded positions in an RNA secondary structure
consensus. A covariance model has nine types of states with seven
kinds of production rules:

\begin{tabular}{lllll}
State  & Description             &  Production             & Emission & Transition\\ \hline
MP  & (pair emitting)           & $V_P \rightarrow a Y b$ & $e_v(a,b)$ & $t_v(y)$  \\
ML, IL & (left emitting)       & $V_L \rightarrow a Y$   & $e_v(a)$   & $t_v(y)$  \\
MR, IR & (right emitting)      & $V_R \rightarrow Y a$   & $e_v(a)$   & $t_v(y)$  \\
B &  (bifurcation)    & $V_B \rightarrow Y_S Z_S$  & 1     &     1     \\
D & (delete)         & $V_D \rightarrow Y$     &    1     &   $t_v(y)$  \\
S & (start)          & $V_S \rightarrow Y$     &    1     &   $t_v(y)$  \\
E & (end)            & $V_E \rightarrow \epsilon$ & 1     &     1     \\
\end{tabular}

For example, a consensus (``match'') pair (MP) state produces two
correlated letters $a$ and $b$ (e.g. a Watson/Crick base pair) and
transits to one of several possible new states $Y$ of various types.
A bifurcation (B) state splits into two new start (S) states Y$_{S}$
and Z$_{S}$ with probability 1.  The E state is a special case that
terminates a derivation ($\epsilon$ represents the empty string). The
reason for separate ML and IL states instead of just L is to
distinguish between consensus (``match'') states and non-consensus
``insert'' states.

Each overall production probability is the product of an emission
probability $e_v$ and a transition probability $t_v$. This is an
independence assumption. The analogous assumption is made by hidden
Markov models, which are a special case of stochastic regular
grammars.

\subsubsection{From consensus structural alignment to guide tree}

Figure~\ref{fig:input_alignment} shows an example input file: a
multiple sequence alignment of homologous RNAs, with a line describing
the consensus RNA secondary structure. The first step of building a CM
is to produce a binary \emph{guide tree} of \emph{nodes} representing
the consensus secondary structure. The guide tree is a parse tree for
the consensus structure, with nodes as nonterminals and alignment
columns as terminals.

\begin{figure}
\includegraphics{Figures/input_alignment}
\caption{input alignment.ai}
\end{figure}


The guide tree has eight types of nodes:

\begin{tabular}{lll}
Node      & Description      &  Production           \\ \hline
MATP  & (pair)                 & $V_{\mbox{\tiny MATP}} \rightarrow a Y b$  \\
MATL  & (single strand, left)  & $V_{\mbox{\tiny MATL}} \rightarrow a Y$   \\
MATR  & (single strand, right) & $V_{\mbox{\tiny MATR}} \rightarrow Y a$   \\
BIF   & (bifurcation)          & $V_{\mbox{\tiny BIF}}  \rightarrow
Y_{\mbox{\tiny BEGL}} Z_{\mbox{\tiny BEGR}}$ \\
ROOT  & (root)                 & $V_{\mbox{\tiny ROOT}} \rightarrow Y$       \\
BEGL  & (begin, left)          & $V_{\mbox{\tiny BEGL}} \rightarrow Y$       \\
BEGR  & (begin, left)          & $V_{\mbox{\tiny BEGR}} \rightarrow Y$       \\
END   & (end)                  & $V_{\mbox{\tiny END}}  \rightarrow \epsilon$ \\ \hline
\end{tabular}
 
These node types correspond closely with a CM's final state types.
The guide tree deals with the consensus structure; for individual
sequences we will need to deal with insertions and deletions with
respect to this consensus. The guide tree is the skeleton on which we
will later organize the CM.

The input alignment is first used to construct a consensus secondary
structure (Figure~\ref{fig:cm_nodetree}, left) that defines which
aligned columns will be ignored as non-consensus (and later modeled as
insertions relative to the consensus), and which consensus alignment
columns are base-paired to each other.

\begin{figure}
\includegraphics{Figures/cm_nodetree}
\caption{cm nodetree.ai}
\end{figure}


Given the consensus structure, consensus base pairs are assigned to
MATP nodes and consensus unpaired columns are assigned to MATL or MATR
nodes. One ROOT node is used at the head of the tree.  Multifurcation
loops and/or multiple stems are dealt with by assigning one or more
BIF nodes that branch to subtrees starting with BEGL or BEGR head
nodes. (ROOT, BEGL, and BEGR start nodes are labeled differently
because they will be expanded to different groups of states; this has
to do with avoiding ambiguous parse trees for individual sequences, as
will soon become apparent.) Alignment columns that are considered to
be insertions relative to the consensus structure are ignored at this
stage.

There will in general be more than one possible guide tree for any
given consensus structure. Almost all of this ambiguity is eliminated
by three conventions: (1) only MATL nodes are used for hairpin loops;
(2) in describing interior loops, MATL nodes are used before MATR
nodes; and (3) BIF nodes are only invoked where necessary to explain
branching secondary structure stems (as opposed to unnecessarily
bifurcating in single stranded sequence). One source of ambiguity
remains. In invoking a bifurcation to explain alignment columns $i..j$
by two substructures on columns $i..k$ and $k+1..j$, there may be
several possible choices of $k$ (e.g. in a single stranded region
between the two substructures). The choice of bifurcation point $k$
impacts the performance of the divide and conquer algorithm.  My
current strategy is to choose $k$ such that $i..k$ and $k+1..j$ are as
close to the same length as possible.

The result of this procedure is the guide tree. The nodes of the guide
tree are numbered in post-traversal order (from root to leaves, so
parent nodes always have lower indices). The guide tree corresponding
to the input multiple alignment in Figure~\ref{fig:input_alignment} is
shown on the right in Figure~\ref{fig:cm_nodetree}.

\subsubsection{From guide tree to covariance model}

A CM must deal with insertions and deletions in individual sequences
relative to the consensus structure. For example, for a consensus base
pair, either partner may be deleted leaving a single unpaired residue,
or the pair may be entirely deleted; additionally, there may be
inserted nonconsensus residues between this pair and the next pair in
the stem. Accordingly, each node in the master tree is expanded into
one or more \emph{states} in the CM as follows:

\begin{tabular}{clccc}
Node   &  States             & nstates & nsplit & ninsert \\ \hline
MATP   & [MP ML MR D] IL IR  &   6     &   4    &  2   \\
MATL   & [ML D] IL           &   3     &   2    &  1   \\
MATR   & [MR D] IR           &   3     &   2    &  1   \\
BIF    & [B]                 &   1     &   1    &  0   \\
ROOT   & [S] IL IR           &   3     &   1    &  2   \\
BEGL   & [S] IL              &   1     &   1    &  0   \\
BEGR   & [S] IL IR           &   2     &   1    &  1   \\
END    & [E]                 &   1     &   1    &  0   \\ \hline
\end{tabular}

The states are grouped into a \emph{split set} of 1-4 states (shown in
brackets above) and an ``insert set'' of 0-2 insert states. The split
set includes the consensus state, which by convention is first. One
and only one of the states in the split set must be visited in every
parse tree (and this fact will be exploited by the divide and conquer
algorithm). The insert state(s) are not obligately visited, and they
have self-transitions, so they will be visited zero or more times in
any given parse tree.

State transitions are then assigned as follows. For bifurcation nodes,
the B state makes transitions to the S states of the child BEGL and
BEGR nodes. Each state in a split set of a non-bifurcation node makes
a possible transition to every insert state in the \emph{same} node,
and to every state in the split set of the \emph{next} node. An IL
state makes a transition to itself, to the IR state in the same node
(if present), and to every state in the split set of the next node. An
IR state makes a transition to itself and to every state in the split
set of the next node.

This arrangement of transitions guarantees that there is unambiguously
one and only one parse tree for any given individual structure. This
is important, because the algorithm will find a maximum likelihood
parse tree for a given sequence, and we wish to interpret this result
as a maximum likelihood structure.

As a convenient side effect of the ordering of the states, it is
guaranteed that the transitions from any state are to a
\emph{contiguous} set of child states, so the transitions for state
$v$ may be kept as an offset and a count; e.g. state $v$ makes
transitions to states $v+\mbox{offset}_v..v+\mbox{count}_v-1$, with offset
always $\geq 0$.  Similarly, the parent states which may transition
into $v$ are also guaranteed to be contiguous in the final CM. This
contiguity is exploited by the implementation.

Thus the final CM structure is an array of M states, connected as a
directed graph by transitions $t_v(y)$ (or probability 1 transitions
$v \rightarrow (y,z)$ for bifurcations) with the states numbered such
that $y,z \geq v$. The numbering of the states is in postorder
traversal with respect to the guide tree. There are no cycles in the
directed graph other than cycles of zero length (e.g. the
self-transitions of the insert states). This is important; since all
the transition dependencies run in one direction, we can do an
iterative dynamic programming calculation through the model states,
starting with the last numbered end state $M$ and ending in the root
state $1$.  An example CM, corresponding to the input alignment of
Figure~\ref{fig:input_alignment}, is shown in
Figure~\ref{fig:cm_graph}.

\begin{figure}
\includegraphics{Figures/cm_graph}
\caption{cm graph.ai}
\end{figure}

\subsubsection{Parameterization}

Using the guide tree and the final CM, each individual sequence in the
input multiple alignment can be converted unambiguously to a CM parse
tree, as shown in Figure~\ref{fig:parsetrees}. Counts for observed
state transitions and singlet/pair emissions are then collected from
these parse trees. The observed counts are converted to transition and
emission probabilities by standard procedures (I calculate maximum a
posteriori parameters, using Dirichlet priors).

\begin{figure}
\includegraphics{Figures/parsetrees}
\caption{parsetrees.ai}
\end{figure}

\subsubsection{Comparison to profile HMMs}

The relationship between an SCFG and a covariance model is analogous
to the relationship of hidden Markov models (HMMs) and profile HMMs
for modeling multiple sequence alignments
\cite{Krogh94,Durbin98,Eddy98}. A comparison may be instructive, at
least to readers familiar with profile HMMs.  A profile HMM is a
repetitive HMM architecture that associates each consensus column of a
multiple alignment with a single type of model node -- a MATL node, in
the above notation. Each node contains a ``match'', ``delete'', and
``insert'' HMM state -- ML, IL, and D states, in the above notation.
The profile HMM also has special begin and end states. Profile HMMs
are therefore essentially a special case of CMs. An unstructured RNA
multiple alignment would be modeled by a guide tree of all MATL nodes,
and converted to an unbifurcated CM that would essentially be
identical to a profile HMM. (The only difference is trivial; the CM
root node includes a IR state, whereas the start node of a profile HMM
does not.) All the other node types (especially MATP, MATR, and BIF)
and state types (e.g. MP, MR, IR, and B) are SCFG augmentations
necessary to extend profile HMMs to deal with RNA secondary structure.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Divide and conquer algorithm}

\subsubsection{Notation}

I use $v$, $w$, $y$, and $z$ as indices of states in the model. These
will range from $1..M$, for a CM with $M$ states. $S_v$ refers to the
\emph{type} of state $v$; it will be one of the nine types
\{D,P,ML,IL,MR,IL,S,E,B\}. $C_v$ is a list of children for state $v$
(states $y$ that $v$ can transit to); it will contain up to six
contiguous indices $y$ with $v <= y <= M$. $P_v$ is a list of parents
for state $v$ (states that could have transited to state $v$); it will
contain up to six contiguous indices $y$ with $1 <= y <= v$. (Because
the sets are guaranteed to be contiguous in the model, the
implementation stores these as an offset relative to $v$ and a count,
as described above.) I use $g$, $h$, $i$, $j$, $k$, $p$, and $q$ as
indices referring to positions in a sequence $x$. These indices range
from $1..L$, for a sequence of length $L$.

The Inside/CYK algorithm calculates $\alpha_v(i,j)$, the log
probability of the most likely CM parse subtree rooted at state $v$
that generates subsequence $x_i..x_j$ of sequence $x$.  The
Outside/CYK algorithm calculates $\beta_v(i,j)$ -- the log probability
of the best alignment of the CM and sequence $x$ \emph{excluding} the
best alignment for the CM subgraph rooted at $v$ generating the
subsequence $x_i..x_j$.  For any chosen cell in the 3D dynamic
programming matrices with coordinates $v,i,j$, $\alpha_v(i,j) +
\beta_v(i,j)$ gives us the score of the best alignment that passes
through that cell. I will refer to $j$ and $i$ as row and column
coordinates for these DP matrices, respectively. I will refer to $v$
as \emph{deck} coordinates in the three-dimensional matrices.
$\alpha_{v}$ and $\beta_{v}$ refer to whole two-dimensional decks
containing scores $\alpha_v(i,j)$ and $\beta_v(i,j)$. The dividing and
conquering will be done in this dimension, by choosing particular
decks as split points.

\subsubsection{inside and inside$^{\mathcal{T}}$}

The Inside/CYK algorithm recursively calculates $\alpha_v(i,j)$ -- the
log probability of the most likely CM parse subtree rooted at state
$v$ that generates subsequence $x_i..x_j$ of sequence $x$. The
calculation initializes at the smallest subgraphs and subsequences
(e.g. subgraphs rooted at E states, generating subsequences of length
0), and iterates outwards to progessively longer subsequences and
larger CM subgraphs. 

For example, if we're calculating $\alpha_v(i,j)$ and $v$ is a P
(pair) state, $v$ will generate the pair $x_i,x_j$ and transit to a
new state $y$ (one of its possible transitions $C_v$) which will have
to account for the smaller subsequence $x_{i+1}..x_{j-1}$. The log
probability for a particular choice of next state $y$ is the sum of
three terms: an emission term $\log e_v(x_i,x_j)$, a transition term
$\log t_v(y)$, and an already calculated solution for the smaller
optimal parse tree rooted at $y$, $\alpha_y(i+1,j-1)$. The answer for
$\alpha_v(i,j)$ is then the maximum of these, for all possible choices
of child states $y$ that $v$ can transit to.

The \texttt{inside} algorithm is as follows:

\begin{algorithm}
\alginout{A CM subgraph $G^r_z$ and subsequence $x_g..x_q$.}
         {Scoring matrix decks $\alpha_r..\alpha_z$.}
\algname{inside}{r,z; g,q} 
\begin{algtab}
\algforto{$v \leftarrow z$}{$r$}
  \algforto{$j \leftarrow g-1$}{$q$} 
    \algforto{$i \leftarrow j+1$}{$g$}
       $d \leftarrow j-i+1$\\
       \algif{$S_v =$ D or S:}
       	$\alpha_v(i,j) = \max\limits_{y \in C(v)} \left[ \alpha_y(i,j)  + \log t_v(y) \right]$\\
       \algelseif{$S_v =$ P and $d>=2$:}
	$\alpha_v(i,j) = \log e(x_i, x_j) + \max\limits_{y \in C(v)} \left[ \alpha_y(i+1,j-1) + \log t_v(y) \right]$\\
       \algelseif{$S_v =$ L and $d>=1$:}
        $\alpha_v(i,j) = \log e(x_i) + \max\limits_{y \in C(v)} \left[ \alpha_y(i+1,j)   + \log t_v(y) \right]$\\
       \algelseif{$S_v =$ R and $d>=1$:}
        $\alpha_v(i,j) = \log e(x_j) +      \max\limits_{y \in C(v)} \left[ \alpha_y(i,j-1)   + \log t_v(y) \right]$ \\
       \algelseif{$S_v =$ B:}
        $(y,z) \leftarrow $ left and right S children of state $v$\\
        $\alpha_v(i,j) = \max\limits_k \left[ \alpha_y(i,k) + \alpha_z(k+1,j) \right]$ \\
       \algelseif{$S_v =$ E and $d=0$:}
	$\alpha_v(i,j) = 0$\\
       \algelse
	$\alpha_v(i,j) = -\infty$\\
\end{algtab}
\end{algorithm}

Given a complete sequence $x$ of length $L$ and a CM $G$ of length
$M$, we could call \texttt{inside(1,M; 1,L)}, and $\alpha_1(1,L)$
would contain the log probability of the best parse of the complete
sequence with the complete model.

We do not have to keep the entire $\alpha$ matrix in memory to
calculate this score.  As we reach higher decks $\alpha_v$ in the
three dimensional dynamic programming matrix, our calculations no
longer depend on certain lower decks. A lower deck $y$ can be
deallocated whenever all the parent decks $P(y)$ that depend on it
have been calculated. \footnote{The implementation goes even further
and recycles decks when possible, saving some initialization steps and
many memory allocation calls; for example, since values in all E decks
are identical, only one E deck needs to be calculated and that
precalculated deck can be reused whenever $S_v = E$.}

This deallocation rule has a nice property that the divide and conquer
algorithm takes quiet advantage of when solving smaller subproblems
for CM subgraphs rooted at some state $w$.  When this root state $w$
is an S state, the $\alpha$ matrix returned by \texttt{inside}
contains only one active deck $\alpha_w$. (No lower state $>w$ can be
reached from any state $<w$ without going through $w$, so all lower
decks are deallocated once deck $w$ is completed.) But when the root
state $w$ is the first state in a split set $w..y$ (see below for more
explanation), all (and only) the decks $\alpha_w..\alpha_y$ are active
when \texttt{inside} returns. 

In some cases we want to recover the optimal parse tree itself, not
just its score. The \texttt{inside}$^{\mathcal{T}}$ routine is a
modified version of $\texttt{inside}$. It keeps an additional ``shadow
matrix'' $\tau_v(i,j)$. A $\tau_v(i,j)$ traceback pointer either
records the index $y$ that maximized $\alpha_v(i,j)$ (for state types
D,S,P,L,R) or records the split point $k$ that maximized
$\alpha_v(i,j)$ for a bifurcation (B) state. The $\tau$ shadox matrix
does not use the deallocation rules (and thus,
\texttt{inside}$^{\mathcal{T}}$ can only be called for problems small
enough that they can be solved within our available memory
space). Thus the \texttt{inside}$^{\mathcal{T}}$ routine works by
calling \texttt{inside} in a mode that keeps shadow matrix $\tau$, and
then calls a recursive traceback starting with $v,i,j$:

\begin{algorithm}
\alginout{A shadow matrix $\tau$ for CM subgraph $G^v$ and subsequence $x_i..x_j$.}
         {Optimal parse tree $\mathcal{T}$.}
\algname{traceback}{v,i,j}
\begin{algtab}
  \algif{$S_v =$ E:}
    attach $v$\\
  \algelseif{$S_v =$ S or D:}
    attach $v$ \\
    \algcall{traceback}{$\tau_v(i,j), i, j$}\\
  \algelseif{$S_v =$ P:}
    attach $x_i,v,x_j$\\
    \algcall{traceback}{$\tau_v(i,j), i+1, j-1$}\\
  \algelseif{$S_v =$ L:}
    attach $x_i,v$\\
    \algcall{traceback}{$\tau_v(i,j), i+1, j$}\\
  \algelseif{$S_v =$ R:}
    attach $v,x_j$\\
    \algcall{traceback}{$\tau_v(i,j), i,   j-1$}\\
  \algelseif{$S_v =$ B:}
    $(y,z) \leftarrow $ left and right S children of state $v$\\
    attach $v$\\
    \algcall{traceback}{$y, i, \tau_v(i,j)$}\\
    \algcall{traceback}{$z, \tau_v(i,j)+1, j$}\\
\algend
\end{algtab}
\end{algorithm}

\subsubsection{outside}

The Outside/CYK algorithm recursively calculates $\beta_v(i,j)$, the
log probability of the most likely CM parse tree for a CM generating a
sequence $x_1..x_L$ \emph{excluding} the optimal parse subtree rooted
at state $v$ that accounts for the subsequence $x_i..x_j$. The
calculation initializes with the entire sequence excluded (e.g.
$\beta_1(1,L) = 0$), and iterates inward to progressively shorter and
shorter excluded subsequences and smaller CM subgraphs.

A complete implementation of the Outside/CYK algorithm requires first
calculating the Inside numbers $\alpha$, because they are needed to
calculate $\beta_v$ when the parent of $v$ is a bifurcation
\cite{Lari90,Lari91,Durbin98}. However, the divide and conquer
algorithm only needs to call \texttt{outside} on unbifurcated, linear
CM subgraphs, so the parent of $v$ is never a bifurcation and the
implementation can be streamlined.

\begin{algorithm}
\alginout{An unbifurcated CM subgraph $G^r_z$ and subsequence $x_g..x_q$.}
         {Scoring matrix decks $\beta_r..\beta_z$.}
\algname{outside}{r,z; g,q}
\begin{algtab}
 $\beta_v(i,j) \leftarrow -\infty \forall v,i,j$\\
 $\beta_r(g,q) \leftarrow 0$\\
 \algforto{$v \leftarrow r+1$}{$z$}
   \algforto{$j \leftarrow q$}{$g-1$}
     \algforto{$i \leftarrow g$}{$j+1$}
       $\beta_v(i,j) = \max\limits_{y \in P(v)} \left\{
              \begin{array}{rcl}
              \beta_y(i,j) + \log t_y(v) &:&  S_y = D,S,E \\
              \beta_y(i-1,j+1) + \log t_y(v) + \log e_y(x_{i-1}, x_{j+1}) &:& S_y = P\\
              \beta_y(i-1,j) + \log t_y(v) + \log e_y(x_{i-1}) &:& S_y = L\\
              \beta_y(i,j-1) + \log t_y(v) + \log e_y(x_{j+1})  &:& S_y = R \\
              \end{array} \right.$
\end{algtab}
\end{algorithm}

As with \texttt{inside}, we do not keep the entire $\beta$ matrix in
memory. A deck $\beta_v$ can be deallocated when all child decks
$C(v)$ that depend on the values in $\beta_v$ have been
calculated. This means that if the last deck $z$ is a bifurcation or
end state, $\beta_z$ will be the only active allocated deck when
\texttt{outside} returns. If $z$ is the last state in a split set
$w..z$, all (and only) the split set decks $\beta_w..\beta_z$ will be
active when \texttt{outside} returns.

\subsubsection{Using outside and inside to divide and conquer}

Now, for any chosen state $v$, $\argmax_{i,j} \alpha_{v}(i,j) +
\beta_{v}(i,j)$ will tell us which cell $v,i,j$ the optimal parse tree
passes through, conditional on using state $v$ in the parse. We know
that any parse tree must include all the bifurcation and start states
of the CM, so we know that the optimal alignment \emph{must} use any
chosen bifurcation state $v$ and its child start states $w$ and
$y$. Thus, we are guaranteed that:

\[
   \max_{i,k,j} \beta_{v}(i,j) + \alpha_{w}(i,k) + \alpha_{y}(k+1,j)
\]

is the optimal overall alignment score, and we also know that

\[
      (i,k,j) = \argmax_{i',k',j'}  \beta_{v}(i',j') +
      \alpha_{w}(i',k') + \alpha_{y}(k'+1,j') 
\]

gives us a triplet that identifies three cells that must be in the
optimal alignment -- $(v,i,j)$, $(w,i,k)$, and $(y,k+1,j)$. This
splits the remaining problem into three smaller subproblems -- an
alignment of the sequence $x_{i}..x_{k}$ to a CM subgraph $w..y-1$, an
alignment of the sequence $x_{k+1}..x_{j}$ to a CM subgraph $y..M$,
and an alignment of the two-piece sequence
$x_1..x_{i-1}//x_{j+1}..x_L$ to a CM subgraph $1..v$.

The subproblems are then themselves split, and this splitting can
continue recursively until all the optimal bifurcation points on the
optimal parsetree have been determined.

At this point the remaining alignment subproblems might be small
enough to be solved by straightforward application of the standard
Inside/CYK algorithm . However, this is not guaranteed to be the
case. A second division strategy is needed that does not depend on
splitting at bifurcations.

For the second strategy, we take advantage of the fact that we know
that the optimal parse tree must also include one and only one state
from the split set of each node (e.g. the non-insert states in the
node). Let $N(\bar{v})$ represent a list of the indices for the split
set in the node that includes state $\bar{v}$ (where it is possible
that $\bar{v}$ is not a member of $N(\bar{v})$, if $\bar{v}$ is an
insert state). ($N(\bar{v})$ contains at most 4 states.) Then for any
alignment subproblem we can choose a provisional split point $\bar{v}$
in the midpoint of the model graph. We know that

\[
(v,i,j) = \argmax_{v' \in N(\hat{v}),i',j'} \alpha_{v'}(i',j') + \beta_{v'}(i',j')
\]

gives us a new cell $(v,i,j)$ in the optimal parse tree, and splits
the problem into two smaller problems. This strategy can be applied
recursively all the way down to single nodes. We can therefore
guarantee that we will never need to carry out a full Inside/CYK
alignment algorithm on any subproblem. The most memory-intensive
alignment problem that needs to be solved is the very first split.
The properties of the first split determine the memory complexity of
the algorithm.

Therefore we will have to deal with three types of problems
(Figure~\ref{fig:splitter_schematic}):

\begin{itemize}
\item A \emph{generic problem} means finding the optimal alignment of
a CM subgraph $G_{r..z}$ to a contiguous subsequence $x_g..x_q$. The
subgraph $G_{r..z}$ corresponds to a complete subtree of the CM's
guide tree -- e.g. state $r$ is a start (S), and state $z$ is an end
(E). $G_{r..z}$ may contain bifurcations. The problem is solved in one
of two ways. If $G_{r..z}$ contains no bifurcations, it is solved as a
wedge problem (see below). Else, the problem is subdivided by the
bifurcation-dependent strategy; an optimal triple $(i,k,j)$ is found
for a bifurcation state $v$ and its children $w,y$, splitting the
problem into a V problem and two generic problems.

\item A \emph{wedge problem} means finding the optimal alignment of an
unbifurcated CM subgraph $G_{r..z}$ to a contiguous subsequence
$x_g..x_q$. State $r$ does not have to be a start state (S); it may be
a state in a split set (MP, ML, MR, or D). State $z$ is an end (E).  A
wedge problem is solved by the split set-dependent strategy: an
optimal $(v,i,j)$ is found, splitting the problem into a V
problem and a smaller wedge problem.

\item A \emph{V problem} consists of finding the optimal alignment of
an unbifurcated CM subgraph $G_{r..z}$ to a noncontiguous, two-piece
sequence $x_g..x_h//x_p..x_q$, exclusive of the residues $x_h$ and
$x_p$ (open circles in Figure~\ref{fig:splitter_schematic}).  State
$r$ can be a start state or any state in a split set; the same is true
for $z$. A V problem is solved by a split set-dependent strategy: an
optimal $(v,i,j)$ is found, splitting the problem into two V problems.
\end{itemize}

\begin{figure}
\includegraphics{Figures/splitter_schematic}
\caption{splitter schematic.ai}
\end{figure}

The three recursive splitting algorithms to solve these problems
are as follows:

\subsubsection{generic\_splitter}
\begin{algorithm}
\alginout{A generic problem, for CM subgraph $G_{r..z}$ and
          subsequence $x_{g..q}$.}
         {An optimal parse subtree $\mathcal{T}$.}

\algname{generic\_splitter}{r,z; g,q}
\begin{algtab}
\algif{alignment problem is small:}
  \algreturn \algcall{inside$^\mathcal{T}$}{r,z; g,q}\\
\algelseif{no bifurcation in $G_{r..z}$:}
  \algreturn \algcall{wedge\_splitter}{r,z; g,q}\\
\algelse
   $v   \leftarrow$ lowest numbered bifurcation state in subgraph $G_{r..z}$.\\
   $w,y \leftarrow$ left and right S children of $v$.\\
   $\beta_v \leftarrow$  \algcall{outside}{r,w; g,q}\\
   $\alpha_w \leftarrow$ \algcall{inside}{w,y-1; g,q}\\
   $\alpha_y \leftarrow$ \algcall{inside}{y,z; g,q}\\

   $(i,k,j) \leftarrow \argmax_{i',k',j'} \alpha_w(i',k') + \alpha_y(k'+1,j') + \beta_v(i',j')$ \\

   $\mathcal{T}_1   \leftarrow$ \algcall{V\_splitter}{r,v; g,i; j,q}\\
   $\mathcal{T}_2 \leftarrow$ \algcall{generic\_splitter}{w,y-1; i,k}\\
   $\mathcal{T}_3 \leftarrow$ \algcall{generic\_splitter}{y,z; k+1,j}\\

   Attach S state $w$ of $\mathcal{T}_2$ as left child of B state $v$ in $\mathcal{T}_1$.\\
   Attach S state $y$ of $\mathcal{T}_3$ as right child of B state $v$ in $\mathcal{T}_2$.\\
  
   \algreturn $\mathcal{T}_1$.\\
\algend
\end{algtab}
\end{algorithm}

\subsubsection{wedge\_splitter}
\begin{algorithm}
\alginout{A wedge problem, for unbifurcated CM subgraph $G_{r..z}$ and
          subsequence $x_{g..q}$.}
         {An optimal parse subtree $\mathcal{T}$.}

\algname{wedge\_splitter}{r,z; g,q}
\begin{algtab}
\algif{alignment problem is small:}
  \algreturn \algcall{inside$^\mathcal{T}$}{r,z; g,q}\\
\algelse
  $(w..y) \leftarrow$ a split set chosen from middle of $G_{r..z}$\\
  $(\alpha_w..\alpha_y) \leftarrow$ \algcall{inside}{w,z; g,q}\\
  $(\beta_w..\beta_y)   \leftarrow$ \algcall{outside}{r,y; g,q}\\
  $(v,i,j) \leftarrow \argmax\limits_{v',i',j'} 
	\alpha_{v'}(i',j') + \beta_{v'}(i',j')$\\
  $\mathcal{T}_1 \leftarrow$ \algcall{V\_splitter}{r,v; g,i; j,q}\\
  $\mathcal{T}_2 \leftarrow$ \algcall{wedge\_splitter}{v,z; i,j}\\
  Attach $\mathcal{T}_2$ to $\mathcal{T}_1$ by merging at state $v$.\\
  
  \algreturn $\mathcal{T}_1$.\\
\algend
\end{algtab}
\end{algorithm}

\subsubsection{V\_splitter}
\begin{algorithm}
\alginout{A V problem, for unbifurcated CM subgraph $G_{r..z}$ and
          two-part subsequence $x_{g..h}//x_{p..q}$.}
         {An optimal parse subtree $\mathcal{T}$.}

\algname{V\_splitter}{r,z; g,h; p,q}
\begin{algtab}
\algif{alignment problem is small:}
  \algreturn \algcall{vinside$^\mathcal{T}$}{r,z; g,h; p,q}\\
\algelse
  $(w..y) \leftarrow$ a split set chosen from middle of $G_{r..z}$\\
  $(\alpha_w..\alpha_y) \leftarrow$ \algcall{vinside}{w,z; g,h; p,q}\\
  $(\beta_w..\beta_y)   \leftarrow$ \algcall{voutside}{r,y; g,h; p,q}\\
  $(v,i,j) \leftarrow \argmax_{v'=w..y, i'=g..h, j'=p..q} 
	\alpha_{v'}(i',j') + \beta_{v'}(i',j')$\\
  $\mathcal{T}_1 \leftarrow$ \algcall{V\_splitter}{r,v; g,i; j,q}\\
  $\mathcal{T}_2 \leftarrow$ \algcall{V\_splitter}{v,z; i,h; p,j}\\
  Attach $\mathcal{T}_2$ to $\mathcal{T}_1$ by merging at state $v$.\\
  \algreturn $\mathcal{T}_1$.\\
\algend
\end{algtab}
\end{algorithm}

\subsubsection{vinside and voutside}

The \texttt{vinside} and \texttt{voutside} routines are just
\texttt{inside} and \texttt{outside}, modified to deal with a
two-piece subsequence $x_g..x_h//x_p..x_q$ instead of a contiguous
sequence $x_g..x_q$. These modifications are fairly obvious.  Most
obviously, the range of $i,j$ is restricted so that $i<=h$ and $j>=p$.
Also, \texttt{vinside(w,z; g,h; p,q)} initializes $\alpha_z(h,p) = 0$:
that is, we know that sequence $x_h..x_p$ has already been accounted
for by a CM parse tree rooted at $z$. 

\subsubsection{Implementation}

In the description of the algorithms above, some technical detail has
been omitted - in particular, a detailed description of efficient
initialization steps, and details of how the matrix coordinates can be
transformed for more highly optimized memory usage. (\texttt{inside}
and \texttt{outside} are actually implemented in a so-called $v,j,d$
coordinate system, where $d = j-i+1$ is the length of the subsequence
$x_i..x_j$; \texttt{vinside} and \texttt{voutside} are implemented in
a so-called $v,j,i$ coordinate system.) 

These details are not necessary for a high level understanding of the
divide and conquer algorithm, but are necessary for reproducing a
working implementation. Full source code for a reference
implementation is therefore freely available at
\url{http://www.genetics.wustl.edu/eddy/infernal/} under a GNU General
Public License. This is an extensively commented ANSI C codebase that
has been tested on GNU/Linux platforms.

In this codebase, the CM data structure is defined in
\texttt{structs.h}. The CM construction procedure is in
\texttt{modelmaker.c:Handmodelmaker()}. The guide tree is constructed
in \texttt{HandModelmaker()}. A CM is constructed from the guide tree
by \texttt{cm\_from\_master()}. Individual parse trees are constructed
using the guide tree by \texttt{transmogrify()}.

The divide and conquer algorithm is implemented in
\texttt{smallcyk.c:CYKDivideAndConquer()}, which will recursively call
a set of functions: the three splitting routines
\texttt{generic\_splitter()}, \texttt{wedge\_splitter()}, and
\texttt{V\_splitter}; the four alignment engines \texttt{inside()},
\texttt{outside()}, \texttt{vinside()}, and \texttt{voutside()}; and
the two traceback routines \texttt{insideT()} and \texttt{vinsideT()}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{distilled}
\end{document}



