\section{Infernal 1.1's profile/sequence comparison pipeline}
\label{section:pipeline}
\setcounter{footnote}{0}

In this section, we briefly outline the processing pipeline for a
single profile/sequence comparison.\footnote{Code gurus and
masochists: you can follow along in \ccode{src/cm\_pipeline.c}.} POINT
OUT SPECIFIC FUNCTIONS??  This
should help give you a sense of what Infernal is doing under the hood,
what sort of mistakes it may make, and what the various results in the
output actually mean. Infernal's pipeline is based closely on the
profile HMM/sequence comparison pipeline in HMMER3
(\url{hmmer.org}). In fact, the first several stages of the
pipeline use code from HMMER's pipeline to score candidate sequences
with profile HMMs that act as filters for the later, more
computationally expensive CM stages.

In briefest outline, the comparison pipeline takes the following
steps:

\begin{description}
\item[Profile HMM filter stages:] The first several stages of the
  pipeline use only a profile HMM, putting off all calculations with
  the CM until later. Since profile HMM algorithms are less complex
  than CM ones, this saves us time by only using the expensive CM
  methods for regions the HMM has given high scores. The profile HMM
  filter stages are very closely based on the similar steps in
  HMMER3's accelerated comparison pipeline with the important
  difference that both \emph{local} and \emph{glocal} versions of HMM
  algorithms are used.
\begin{description}
\item[\textbf{Null model.}] Calculate a score term for the ``null
  hypothesis'' (a probability model of \emph{non-}homology). This
  score correction is used to turn all subsequent profile/sequence bit
  scores into a final log-odds bit score.
  
\item[\textbf{Local SSV filter.}] The SSV (``Single Segment Viterbi'')
  algorithm looks for high-scoring \emph{ungapped} alignments. Each
  sequence segment (referred to as a ``diagonal'') in a SSV alignment
  is extended slightly to define a window. Overlapping windows are
  merged together into a single window. Then, long windows greater
  than a maximum length $2L$ (where $L$ is the maximum of the model's
  $W$ parameter\footnote{$W$ is the expected maximum hit length for
  the model calculated by \prog{cmbuild} and stored in the CM file}
  and $1.25$ times the consensus length of the model) are split into
  multiple windows of length $2L$ with $L-1$ overlapping residues
  between adjacent windows. Each window is then passed on to the next
  next pipeline step. Any nucleotides that are not contained in an SSV
  window are not evaluated further.

\item[\textbf{Local Viterbi filter.}] A more stringent accelerated
  filter. An optimal (maximum likelihood) gapped alignment score is
  calculated for each sequence window that survived SSV. If this score
  passes a set threshold, the sequence passes to the next step; else
  it is rejected.

\item[\textbf{Bias filter.}] A hack that reduces false positive
  Viterbi hits due to biased composition sequences. A two-state HMM is
  constructed from the mean residue composition of the profile and the
  standard residue composition of the null model, and used to score
  the sequence window. The Viterbi bit score is corrected using this
  as a second null hypothesis. If the Viterbi score still passes the
  Viterbi threshold, the sequence passes on to the next step; else it
  is rejected. \emph{The bias filter score correction will also be applied
  to the local Forward filter and glocal Forward filter scores that
  follow.}
  
\item[\textbf{Local Forward filter.}] The full likelihood of
  the profile/sequence window comparison is evaluated, summed over the
  entire alignment ensemble, using the HMM Forward algorithm with the
  HMM in \emph{local} mode. This score is corrected to a bit score using the
  null model and bias filter scores. If the bit score passes a set
  threshold, the sequence window passes on to the next step; else it
  is rejected.

\item[\textbf{Glocal Forward filter/parser.}]  The HMM Forward
  algorithm is again used to evaluate the full likelihood of the
  profile/sequence window comparison, but this time the HMM is
  configured in \emph{global} mode, which requires that any valid
  alignment begin at the first consensus position (match or delete)
  and end at the final consensus position (in local mode alignments
  can begin and end at any model position). Aligments in this stage
  (like all previous stages) can be local with respect to the sequence
  (starting and ending at any sequence position), which is why this
  stage is referred to as \emph{glocal}: global with respect to the
  model and local with respect to the sequence. The glocal Forward
  score is corrected to a bit score using the null model and bias
  filter scores. If the bit score passes a set threshold, the sequence
  window passes on to the next step; else it is rejected.
  
\item[\textbf{Glocal envelope definition.}] Using the glocal Forward
  parser results, now combined with a glocal Backward parser,
  posterior probabilities that each residue in the window is aligned
  to a position of the model are calculated. A discrete set of
  putative alignments is identified by applying heuristics to
  posterior probabilities. This procedure identifies \emph{envelopes}:
  subsequences in the target sequence window which contain a lot of
  probability mass for a match to the profile. The envelopes are often
  significantly shorter than the full window, containing only residues
  that have a signficant probability of aligning to the HMM, which is
  critical for the subsequent CM stages of the pipeline. Each
  envelope's (there can be more than one if the evaluation reveals
  multiple full length alignments in a single window) glocal Forward
  score is corrected to a bit score using the null model and bias
  filter scores. If the bit score passes a set threshold, the sequence
  envelope passes on to the next step; else it is rejected.

\end{description}

\item[Covariance model stages:] The remainder of the pipeline uses the
  CM to evaluate each envelope that survived the profile HMM filter
  stages.

\item[\textbf{HMM banded CYK filter.}] For each envelope, posterior
  probabilities that each sequence residue aligns to each state of a
  profile HMM is computed\footnote{The profile HMM used at this stage
  is referred to as a CM plan 9 (CP9) HMM in it the code. It is
  similar but not identical to the one used for filtering. It was
  constructed to be maximally similar to the CM and includes
  transitions between insert and delete states not allowed in the
  HMMER plan 7 models used in the filtering steps. CP9 HMMs are
  essentially a reimplementation of the maximum likelihood heuristic
  HMM described in \cite{WeinbergRuzzo06}.} and used to derive bands
  (constraints) for the CM CYK algorithm \cite{Brown00,
  Nawrocki09}. A banded version of the CYK algorithm is used to
  determine the bit score of the maximum likelihood alignment of any
  subsequence within the envelope to the CM that is consistent with
  the HMM-derived bands. If this score passes a set threshold, the
  sequence envelope passes on to the next step; else it is rejected. 
  Additionally, the boundaries of the envelope may be modified
  (shortened: the start position increased and/or the end position
  decreased) at this stage, as detailed below.

\item[\textbf{HMM banded Inside parser.}] The full likelihood of the
  profile/sequence envelope is now evaulated, summed over the entire
  alignment ensemble for every subsequence of the envelope, using the
  CM Inside algorithm. Again HMM bands are used to constrain the
  CM dynamic programming calculations. This procedure identifies zero
  or more non-overlapping \emph{hits}, defined as subsequences in
  envelope. An \emph{ad hoc} ``null3'' hypothesis is constructed for
  each hit's composition and used to calculate a biased composition
  score correction. The null3 score is subtracted from the Inside bit
  score and an E-value is computed for that score.

\item[\textbf{Alignment.}] For each identified hit, the HMM banded
  Inside/Outside algorithm is performed and an optimal accuracy (also
  sometimes called maximum expected accuracy) alignment is
  calculated. 

\item[\textbf{Storage.}] Now we have a \emph{hit score} (and E-value)
  and each hit has an optimal accuracy alignment annotated with
  per-residue posterior probabilities.

\end{description}

\subsection{Filter thresholds are dependent on the size of the search space}
Before we go into more detail on each stage, we'll briefly discuss the 
filter thresholds used for each stage of the pipeline. Unlike in
HMMER, in which the filter thresholds are always the same regardless of the query
and target, in Infernal, the HMM filter thresholds are dependent on
the size of the search space. For \prog{cmsearch}, the search space is
the size of the target sequence databases, in total number of
nucleotides. For \prog{cmscan}, the search space is the length of the
query sequence in nucleotides multiplied by the number of models in
the target CM database. 

In general, the larger the search space, the stricter the filter
thresholds become. The specific thresholds used for all stages of the
pipeline for all possible search space sizes ($Z$) are given in
Table~\ref{tbl:thresholds}.

The rationale for making filter thresholds more strict as $Z$
increases is as follows. As $Z$ increases, so too does the CM bit
score necessary for a hit to reach the E-value reporting threshold (by
default 10.0). If we then assume that a hit's filter HMM score
increases with its CM score (which should be true in most cases), then
it follows that we should be able to decrease filter P-value
thresholds (increase filter score thresholds) as $Z$ increases without
sacrificing sensitivity relative to an unfiltered search. Of course,
it's unclear exactly how much we can decrease the thresholds by. We've
taken an empirical approach by measuring performance on an internal
benchmark of remote structural RNA homology search based on Rfam. The
sets of filter thresholds in Table~\ref{tbl:thresholds} were
determined to achieve what we feel is a good trade-off between
sensitivity, specificity and speed on that benchmark.

% Source of table:
% ~nawrockie/notebook/12_0326_talk_xfam/latex/xfam.tex
% 'Avg relative speed' row calc'ed using relative rmark3 times
% from 'Avg model per Gb time' row of the version of the
% table in ~nawrockie/notebook/12_0326_talk_xfam/latex/xfam.tex,
% and rounding.
%
\begin{table}
\begin{tabular}{ll||r|r|r|r|r|r|}
filter &               &         & 2Mb     & 20Mb     & 200Mb  & 2Gb     &          \\
stage  & model         & $<$ 2Mb & to 20Mb & to 200Mb & to 2Gb & to 20Gb & $>$ 20Gb \\  \hline
& & & & & & & \\
F1 (MSV/SSV) & HMM & 0.35 & 0.35 & 0.35 & 0.15 & 0.15 & 0.06 \\
& & & & & & & \\
%\multicolumn{2}{c||}{\emph{windows $>$ 2*W are split up}} & & & & & & \\
%& & & & & & & \\
F2 (Viterbi) & HMM & off & off & 0.15 & 0.15 & 0.15 & 0.02 \\
& & & & & & & \\
F3 (Forward) & HMM & 0.02 & 0.005 & 0.003 & 8E-4 & 2E-4 & 2E-4 \\
& & & & & & & \\
F4 (glocal Forward) & HMM & 0.02 & 0.005 & 0.003 & 8E-4 & 2E-4 & 2E-4 \\
& & & & & & & \\
%\multicolumn{2}{c||}{\emph{overlapping windows are merged}} & & & & & & \\
%& & & & & & & \\
F5 (glocal envelope defn)  & HMM & 0.02 & 0.005 & 0.003 & 8E-4 & 2E-4 & 2E-4 \\
& & & & & & & \\
F6 (HMM banded CYK)  & CM & 1E-4 & 1E-4 & 1E-4 & 1E-4 & 1E-4 & 1E-4 \\
& & & & & & & \\
Final (HMM banded Inside)& CM &  & & & & & \\ \hline
& & & & & & & \\
& & & & & & & \\
%Avg model per Gb time   & & 3.48h & 1.25h & 0.54h & 0.27h & 0.18h & 0.11h \\
Avg relative speed   & & 30.0 & 10.0 & 5.0 & 2.5 & 1.5 & 1.0 \\
\end{tabular}
\end{table}

\subsection{Manually setting filter thresholds}

As described above, by default filter thresholds are automatically set
based on the search space $Z$, but several options exist for
overriding these defaults. There are six pre-defined sets of
thresholds, each available via a single command-line option to
\prog{cmsearch} or \prog{cmscan}. In order from least strict to most
strict these are: \ccode{--max}, \ccode{--nohmm}, \ccode{--mid}, and
\ccode{--rfam}. (The default thresholds lie somewhere between
\ccode{--mid} and \ccode{--rfam}.) Additionally, you can specify that
the filter thresholds be set as if the search space was \ccode{<x>}
megabases with the \ccode{--FZ <x>} option. More detail on each of
these options is provided below. A one-line description of each option
is printed as part of the \prog{cmsearch} and \prog{cmscan} 'help'
output that gets displayed with the use of the \ccode{-h}
option. There's also descriptions of these options in the
\prog{cmsearch} and \prog{cmscan} manual pages.

\begin{description}
\item[\ccode{--max}:] Turn off all filters, run Inside on full length
  target sequences. This option will result in maximum sensitivity but
  is also the slowest. Using this option will slow down
  \prog{cmsearch} by about 3 or 4 orders of magnitude.
\item[\ccode{--nohmm}:] Turn off all profile HMM filters, run only the
  CYK filter on full length sequences and Inside on surviving
  envelopes. Searches with this option will often be between 5 and
  times faster than \ccode{--max} searches.
\item[\ccode{--mid}:] Set all profile HMM filter thresholds (SSV,
  local Viterbi, local Forward, glocal Forward and glocal domain
  definition) to the same, set P-value. By default this P-value is
  $0.02$, but it is settable to \ccode{<x>} by also using the
  \ccode{--Fmid <x>} option. These searches will 
\item[\ccode{--rfam}:] Set all filter thresholds as if the search
  space were more than 20 Gb. These are the filter thresholds used by
  a database like Rfam, which annotates RNAs in an EMBL-based sequence
  dataset that is several hundred Gb. If you're trying to
  reproduce results in future versions of Rfam that use Infernal 1.1
  (of course, at the time of writing, no version of Rfam yet exists
  which has used version 1.1) you probably want to use this option.
  This option will have no effect if the target search space is more
  than 20 Gb. 
\item[\ccode{--FZ <x>}:] Set the filter thresholds as if the search space
  were \ccode{<x>} Mb instead of its actual size. Importantly, the E-values
  reported will still correspond to the actual size. (To change the
  search space size the E-values correspond to, use the \ccode{-Z <x2>}
  option.\footnote{Note that if you use \ccode{-Z <x2>} without
  \ccode{--FZ <x>}, the filter thresholds will be set as if the
  search space size was \ccode{<x2>} Mb.
\end{description}

For expert users, options also exist to precisely set each individual
filter stage's threshold. These are \ccode{--F1}, \ccode{--F1b},
\ccode{--F2}, \ccode{--F2b}, \ccode{--F3}, \ccode{--F3b},
\ccode{--F4}, \ccode{--F4b}, \ccode{--F5}, \ccode{--F5b}, and
\ccode{--F6}. Additional options exist for turning on or of each stage
as well: \ccode{--noF1}, \ccode{--doF1b}, \ccode{--noF2},
\ccode{--noF2b}, \ccode{--noF3}, \ccode{--noF3b}, \ccode{--noF4},
\ccode{--noF4b}, \ccode{--noF5}, \ccode{--noF5b}, and
\ccode{--noF6}. Because these options are only expected to be useful
to a small minority of users, they are only displayed in the help
message for \prog{cmsearch} or \prog{cmscan} if the special
\ccode{--devhelp} option is used.

\subsection{HMM-only filter pipeline:}

As mentioned earlier in the guide (specifically in the tutorial
section), Infernal uses a special pipeline for models that have zero
basepairs. For such models, it makes sense to use a profile HMM
instead of a covariance model for searching because HMM algorithms are
more efficient than CM ones and CMs are only more powerful than HMMs
when basepairs are modeled by the CM.

\subsection{In more detail: profile HMM filter stages}

REWRITE THIS SENTENCE: In more detail, each step is described in subsections that follow:

\subsubsection{Null model.}

The ``null model'' calculates the probability that the target sequence
is \emph{not} homologous to the query profile. A profile HMM filter
bit score is the log of the ratio of the sequence's probability
according to the profile (the homology hypothesis) over the null model
probability (the non-homology hypothesis).

The null model is a one-state HMM configured to generate ``random''
sequences of the same mean length $L$ as the target sequence, with
each residue drawn from a background frequency distribution of $0.25$
for all four RNA nucleotides (a
standard i.i.d. model: residues are treated as independent and
identically distributed). 

For technical reasons, the \emph{residue emission} probabilities of
the null model are incorporated directly into
the profile HMM, by turning each emission probability in the profile into
an odds ratio. The null model score calculation therefore is only
concerned with accounting for the remaining \emph{transition}
probabilities of the null model and toting them up into a bit score
correction.  The null model calculation is fast, because it only
depends on the length of the target sequence, not its sequence.

\subsubsection{SSV filter.}

The sequence is aligned to the profile using a specialized model that
allows a single high-scoring local ungapped segment to match.  The
optimal alignment score (Viterbi score) is calculated under this
specialized model, hence the term SSV, for ``single-segment
Viterbi''. SSV is similar, but not identical to, the MSV
(``multi-segment Viterbi) algorithm used by the programs in HMMER used
for protein sequence analysis. There are two main differences. First,
SSV only allows a single ungapped segment match between the sequence
and specialized model. Second, the variant of SSV used by Infernal is
designed for scanning along potentially long sequences (think
chromosomes instead of protein sequences) and potentially finding many
high-scoring hits in each sequence. This scanning SSV algorithm was
developed by Travis Wheeler for a soon-to-be released version of HMMER
that includes a tool for DNA homology search.

Vector parallelization methods are used to accelerate optimal ungapped
alignment in SSV.

The P-value threshold for what subsequences pass this filter range
from $0.35$ for small target databases down to $0.06$ for large ones
(Table~\ref{tbl:thresholds}). Take as an example, the \prog{cmsearch} for tRNAs
performed in the tutorial. The database size was roughly 6 Mb, so the
SSV threshold was set as $0.35$. This means that about $35\%$ of
nonhomologous sequence is expected to pass. 




By default, comparisons with a P-value of $\leq$ 0.02 pass this
filter, meaning that about $2\%$ of nonhomologous sequences are
expected to pass. You can use the \ccode{--F1 <x>} option to change
this threshold. For example, \ccode{--F1 <0.05>} would pass 5\% of the
comparisons, making a search more sensitive but slower. Setting the
threshold to $>1.0$ (\ccode{--F1 99} for example) assures that all
comparisons will pass. Shutting off the MSV filter may be worthwhile
if you want to make sure you don't miss comparisons that have a lot of
scattered insertions and deletions. Alternatively, the \ccode{--max}
option causes the MSV filter step (and all other filter steps) to be
bypassed.

The MSV bit score is calculated as a log-odds score using the null
model for comparison. No correction for a biased composition or
repetitive sequence is done at this stage. For comparisons involving
biased sequences and/or profiles, more than 2\% of comparisons will
pass the MSV filter. At the end of search output, there is a line
like:

\begin{sreoutput}
 Passed MSV filter:                    107917  (0.020272); expected 106468.8 (0.02)
\end{sreoutput}

 which tells you how many and what fraction of comparisons passed the
 MSV filter, versus how many (and what fraction) were expected. 

\subsection{Biased composition filter.}

It's possible for profiles and/or sequences to have biased residue
compositions that result in ``significant'' log-odds bit scores not
because the profile matches the sequence particularly well, but
because the \emph{null model} matches the sequence particularly badly.

HMMER uses fairly good methods to compensate its scores for biased
composition, but these methods are computationally expensive and
applied late in the pipeline (described below).

In a few cases, profiles and/or target sequences are sufficiently
biased that too many comparisons pass the MSV filter, causing HMMER3
speed performance to be severely degraded. Although the final scores
and E-values at the end of the pipeline will be calculated taking into
account a ``null2'' model of biased composition and simple repetition,
the null2 model is dependent on a full alignment ensemble calculation
via the Forward/Backward algorithm, making it computationally complex,
so it won't get calculated until the very end. The treatment of biased
composition comparisons is probably the most serious problem remaining
in HMMER3. Solving it well will require more research. As a stopgap
solution to rescuing most of the speed degradation while not
sacrificing too much sensitivity, an \emph{ad hoc} biased composition
filtering step is applied to remove highly biased comparisons.

On the fly, a two-state HMM is constructed. One state emits residues
from the background frequency distribution (same as the null1 model),
and the other state emits residues from the mean residue composition
of the profile (i.e. the expected composition of sequences generated
by the core model, including match and insert states
[\ccode{p7\_hmm.c:p7\_hmm\_SetComposition()}]). Thus if the profile is
highly biased (cysteine-rich, for example; or highly hydrophobic with
many transmembrane segments), this composition bias will be captured
by this second state. This model's transitions are arbitrarily set
such that state 1 emits an expected length of 400 at a time, and state
2 emits an expected length of M/8 at a time (for a profile of length
M). An overall target sequence length distribution is set to a mean of
$L$, identical to the null1 model.

The sequence is then rescored using this ``bias filter model'' in
place of the null1 model, using the HMM Forward algorithm. (This
replaces the null1 model score at all subsequent filter steps in the
pipeline, until a final Forward score is calculated.) A new MSV bit
score is obtained.

If the P-value of this still satisfies the MSV thresholds, the
sequence passes the biased composition filter. 

The \ccode{--F1 <x>} option controls the P-value threshold for
passing the MSV filter score, both before (with the simple null1
model) and after the bias composition filter is applied.

The \ccode{--max} option bypasses all filters in the pipeline,
including the bias filter.

The \ccode{--nobias} option turns off (bypasses) the biased
composition filter.  The simple null model is used as a null
hypothesis for MSV and in subsequent filter steps. The biased
composition filter step compromises a small amount of sensitivity.
Though it is good to have it on by default, you may want to shut it
off if you know you will have no problem with biased composition hits.

 At the end of a search output, you will see a line like:

\begin{sreoutput}
 Passed bias filter:                   105665  (0.019849); expected 106468.8 (0.02)
\end{sreoutput}

which tells you how many and what fraction of comparisons passed the
biased composition filter, versus how many were expected. (If the
filter was turned off, all comparisons pass.)


\subsection{Viterbi filter.}

The sequence is now aligned to the profile using a fast Viterbi
algorithm for optimal gapped alignment.

This Viterbi implementation is specialized for speed.  It is
implemented in 8-way parallel SIMD vector instructions, using reduced
precision scores that have been scaled to 16-bit integers. Only one
row of the dynamic programming matrix is stored, so the routine only
recovers the score, not the optimal alignment itself. The reduced
representation has limited range; local alignment scores will not
underflow, but high scoring comparisons can overflow and return
infinity, in which case they automatically pass the filter.

The final Viterbi filter bit score is then computed using the
appropriate null model log likelihood (by default the biased
composition filter model score, or if the biased filter is off, just
the null model score). If the P-value of this score passes the Viterbi
filter threshold, the sequence passes on to the next step of the
pipeline.
 
The \ccode{--F2 <x>} option controls the P-value threshold for passing
the Viterbi filter score. The default is 0.001.
The \ccode{--max} option bypasses all filters in the pipeline.


At the end of a search output, you will see a line like:

\begin{sreoutput}
Passed Vit filter:                      2207  (0.00443803); expected 497.3 (0.001)
\end{sreoutput}

which tells you how many and what fraction of comparisons passed the
Viterbi filter, versus how many were expected.
 
  

\subsection{Forward filter/parser.}

The sequence is now aligned to the profile using the full Forward
algorithm, which calculates the likelihood of the target sequence
given the profile, summed over the ensemble of all possible
alignments.

This is a specialized time- and memory-efficient Forward
implementation called the ``Forward parser''. It is implemented in
4-way parallel SIMD vector instructions, in full precision (32-bit
floating point). It stores just enough information that, in
combination with the results of the Backward parser (below), posterior
probabilities of start and stop points of alignments (domains) can be
calculated in the domain definition step (below), although the
detailed alignments themselves cannot be.

The Forward filter bit score is calculated by correcting this score
using the appropriate null model log likelihood (by default the biased
composition filter model score, or if the biased filter is off, just
the null model score). If the P-value of this bit score passes the
Forward filter threshold, the sequence passes on to the next step of
the pipeline.

The bias filter score has no further effect in the pipeline. It is
only used in filter stages. It has \emph{no} effect on final reported
bit scores or P-values. Biased composition compensation for final bit
scores is done by a more complex domain-specific algorithm, described
below.

The \ccode{--F3 <x>} option controls the P-value threshold for passing
the Forward filter score. The default is 1e-5.  The \ccode{--max}
option bypasses all filters in the pipeline.

At the end of a search output, you will see a line like:

\begin{sreoutput}
Passed Fwd filter:                      1076  (0.00216371); expected 5.0 (1e-05)
\end{sreoutput}

which tells you how many and what fraction of comparisons passed the
Forward filter, versus how many were expected.


\subsection{Domain definition.}

A target sequence that reaches this point is very likely to contain
one or more significant matches to the profile. These matches are
referred to as ``domains'', since the main use of HMMER has
historically been to match profile HMMs from protein domain databases
like Pfam, and one of HMMER's strengths is to be able to cleanly parse
a multidomain target sequence into its multiple nonoverlapping hits to
the same domain model.

The domain definition step is essentially its own pipeline, with steps
as follows:\footnote{Code gurus and masochists can follow along in 
\ccode{src/p7\_domaindef.c}.}

\paragraph{Backward parser.}
The counterpart of the Forward parser algorithm is calculated in an
analogous time- and memory-efficient implementation. The Forward
algorithm gives the likelihood of all \emph{prefixes} of the target
sequence, summed over their alignment ensemble, and the Backward
algorithm gives the likelihood of all \emph{suffixes}. For any given
point of a possible model state/residue alignment, the product of the
Forward and Backward likelihoods gives the likelihood of the entire
alignment ensemble conditional on using that particular alignment
point. Thus, we can calculate things like the posterior probability
that an alignment starts or ends at a given position in the target
sequence.

\paragraph{Domain decoding.}
The posterior decoding algorithm is applied, to calculate the
posterior probability of alignment starts and ends (profile B and E
state alignments) with respect to target sequence position.

The sum of the posterior probabilities of alignment starts (B states)
over the entire target sequence is the \emph{expected number of
  domains} in the sequence.

In a tabular output (\ccode{--tblout}) file, this number is in the
column labeled \ccode{exp}.

\paragraph{Region identification.}

A heuristic is now applied to identify a \emph{non-overlapping} set of
``regions'' that contain significant probability mass suggesting the
presence of a match (alignment) to the profile.

For each region, the expected number of domains is calculated (again
by posterior decoding on the Forward/Backward parser results). This
number should be about 1: we expect each region to contain one local
alignment to the profile. 

In a tabular output (\ccode{--tblout}) file, the number of discrete
regions identified by this posterior decoding step is in the column
labeled \ccode{reg}. It ought to be almost the same as the expectation
\ccode{exp}. If it is not, there may be something funny going on, like
a tandem repetitive element in the target sequence (which can produce
so many overlapping weak hits that the sequence appears to be a
significant hit with lots of domains expected \emph{somewhere}, but
the probability is fuzzed out over the repetitive region and few or no
good discrete alignment regions can be identified).

\paragraph{Envelope identification.}

Now, within each region, we will attempt to identify \emph{envelopes}.
An \emph{envelope} is a subsequence of the target sequence that
appears to contain alignment probability mass for a likely domain (one
local alignment to the profile).

When the region contains $\simeq$1 expected domain, envelope
identification is already done: the region's start and end points are
converted directly to the envelope coordinates of a putative domain.

There are a few cases where the region appears to contain more than
one expected domain -- where more than one domain is closely spaced on
the target sequence and/or the domain scores are weak and the
probability masses are ill-resolved from each other. These
``multidomain regions'', when they occur, are passed off to an even
more \emph{ad hoc} resolution algorithm called \emph{stochastic
  traceback clustering}. In stochastic traceback clustering, we sample
many alignments from the posterior alignment ensemble, cluster those
alignments according to their overlap in start/end coordinates, and
pick clusters that sum up to sufficiently high probability. Consensus
start and end points are chosen for each cluster of sampled
alignments. These start/end points define envelopes.

These envelopes identified by stochastic traceback clustering are
\emph{not} guaranteed to be nonoverlapping. It's possible that there
are alternative ``solutions'' for parsing the sequence into domains,
when the correct parsing is ambiguous. HMMER will report all
high-likelihood solutions, not just a single nonoverlapping parse.

It's also possible (though rare) for stochastic clustering to identify
\emph{no} envelopes in the region.

In a tabular output (\ccode{--tblout}) file, the number of regions
that had to be subjected to stochastic traceback clustering is given
in the column labeled \ccode{clu}. This ought to be a small number
(often it's zero). The number of envelopes identified by stochastic
traceback clustering that overlap with other envelopes is in the
column labeled \ccode{ov}. If this number is non-zero, you need to be
careful when you interpret the details of alignments in the output,
because HMMER is going to be showing overlapping alternative
solutions. The total number of domain envelopes identified (either by
the simple method or by stochastic traceback clustering) is in the
column labeled \ccode{env}. It ought to be almost the same as the
expectation and the number of regions.

\paragraph{Maximum expected accuracy alignment.}
Each envelope is now aligned to the profile using the full
Forward/Backward algorithm. The profile is configured to ``unihit''
mode, so that the profile expects only one local alignment (domain) in
the envelope (as opposed to multiple domains).  Posterior decoding is
used to calculate the posterior probability of every detailed
alignment of profile state to sequence residue. The posterior
decodings are used to extract a ``maximum expected accuracy''
alignment. Each aligned residue is annotated with its posterior
probability in the Forward/Backward alignment ensemble.

Currently, the Forward, Backward, and posterior decoding calculations
at this step are \emph{not} memory efficient. They calculate matrices
requiring roughly $36 ML$ bytes, where $M$ is the profile length and
$L$ is the length of the envelope subsequence. Usually in
\prog{hmmsearch} and \prog{hmmscan}, profiles and envelopes are small
enough that this is not a problem. For example, a typical Pfam domain
model is about 200 residues long, matching to individual target
envelopes of about 200 residues each; this requires about 1.4 MB of
memory in MEA alignment. However, in the new \prog{phmmer} and
\prog{jackhmmer} programs, it's often going to be the case that you're
aligning an entire query sequence to an entire target sequence in a
single unresolved ``domain'' alignment. If this is titin (about 40,000
residues), it would require 57.6 GB of RAM. For this reason,
currently, \prog{phmmer} and \prog{jackhmmer} can only handle query
sequences of up to a few thousand residues. If you see a ``fatal
exception'' error complaining about failure of a large memory
allocation, you're almost certainly seeing a prohibitive memory
requirement at this stage.\footnote{We know how to fix this, with
  memory-efficient algorithms. It's just a matter of finding the time
  to do it.}

In a tabular output (\ccode{--tblout}) file, the number of domains in
envelopes (before any significance thresholding) is in the column
labeled \ccode{dom}. This will generally be the same as the number of
envelopes.

\paragraph{Biased composition score correction (``null2'')}
An \emph{ad hoc} biased composition score correction is calculated for
each envelope, using the posterior decoding. A corrected bit score and
P-value for each envelope is calculated. These per-domain scores and
P-values will eventually be subjected to per-domain thresholds to
define significant domains that will appear in output.

Once the position-specific ``null2'' score is available, specifying a
biased composition correction that applies to every residue, the total
corrected bit score for the target sequence is recalculated, by
summing up envelope scores for each significant domain.




