\section{How \texttt{cmsearch} uses filters to accelerate search}

A major limitation to the practical use of CMs is the slow running
time of homology search implementations. \prog{cmsearch} has two
acceleration strategies for alleviating this. In this section we'll
briefly describe these two strategies and how they're employed within
\prog{cmsearch}.

The two acceleration strategies are both filtering approaches.  The
main idea of filtering is to use a fast algorithm to do a first-pass
scan of the database with a wide net, allowing everything that
could possibly be a good hit to survive and eliminating everything
that is very unlikely to be a good hit. After the filter is run, the
expensive CM algorithms are run only on the surviving fraction of the
database.

The first filtering approach uses an HMM as the fast first-pass
algorithm. HMM filtering for CM homology search was introduced by
Weinberg and Ruzzo
\cite{WeinbergRuzzo04,WeinbergRuzzo04b,WeinbergRuzzo06}. We use a
specific type of HMM architecture, essentially a reimplementation of
Weinberg and Ruzzo's maximum-likelihood HMMs \cite{WeinbergRuzzo06},
that we call CM plan 9 HMMs or CP9 HMMs (to distinguish them from the
plan 7, or p7 HMMs of the \software{HMMER} software package
\htmladdnormallink{http://hmmer.janelia.org}{http://hmmer.janelia.org}). HMMs
are unable to model the interactions between base-paired columns that
a CM can model, which makes them less sensitive and specific for RNA
sequence analysis, but in exchange they are more efficient to compute
with, so they're faster than CMs and useful for filtering.

The second filtering approach is a banded dynamic programming
technique called query-dependent banding (QDB). QDB precalculates
regions of the CM dynamic programming matrix that have negligible
probability and can be skipped to save time
\cite{NawrockiEddy07}. This calculation is dependent only on the query
CM itself, it's independent of the database being search, so it only
has to be done once per model. 

\prog{cmsearch} uses both of these filtering strategies in
combination. HMM filtering is faster but less specific than QDB
filtering, so it is used first. Any hit that scores above the HMM
filter threshold survives the HMM filter and is then searched with the
QDB filter. Any hit that surives both filters is reevaluated again
using the Inside algorithm, which is the slowest but most specific
algorithm we have, which determines the final scores of the hits in
the database.

The previous section discussed how \prog{cmcalibrate} calibrates
models, a step which we highly recommend before searching.
\prog{cmsearch} can be run with non-calibrated models but the 
filters are employed differently than for calibrated models.  Next,
we'll discuss how filters are used for searches with calibrated
models, and then we'll discuss how they're used for searches with
non-calibrated models.

\subsection{Filtered searches with calibrated models}

\subsubsection{determining appropriate HMM filter score thresholds with \prog{cmcalibrate}}
A goal of our filtering approach is to sacrifice only a small amount
of sensitivity for the win in speed. In other words, we want faster
searches, but only if they won't miss potential homologs that a slow,
non-filtered search would have found.  Weinberg's rigorous HMM filters
addressed this very nicely, by guaranteeing that all hits above a
certain CM threshold will survive the filter
\cite{WeinbergRuzzo04}. Our approach takes a slightly different tact,
and relaxes the 100\% guarantee to a 99\% probability using a
empirical sampling technique. That is, we estimate our HMM filter will
cause us to miss 1\% of the hits that a non-filtered search would have
found. This sampling technique is performed by \prog{cmcalibrate} and
takes advantage of the CM as a generative probabilistic model.
Sequences are generated, or emitted, from the model and searched with
both an expensive CM algorithm (either CYK or Inside) and the faster
HMM Forward search algorithm. The HMM scores are ranked and the HMM
filter threshold is set as $x$, the 99\% worst score. If we assume
the sample of sequences we drew from the HMM is representative of real
RNA homologs of the family we're modelling, then using an HMM filter
and a cutoff of $x$ will allow 99\% of the real homologs to
survive. Of course, this is a strong and potentially dangerous
assumption to make, but it essentially underlies our entire modelling
approach - we're just assuming our CM is a good model of the family
we're trying to model!  In other words, if this assumption is wrong,
we'll get poor performance for a variety of reasons, and an
inappropriate filter threshold would be a relatively weak concern.  In
the future, as CMs get better due to improved parameterization, etc.,
our assumption will become more true, and this filtering strategy will
improve as well. 

When you calibrate a model with \prog{cmcalibrate} the HMM filter
thresholds are calculated using the sampling technique and printed to
the CM file. (HMM filter threshold determination is one of two
purposes of \prog{cmcalibrate}, the other is fitting exponential tails
for E-value calculation as described in section 5).

\subsubsection{how \prog{cmsearch} sets filter thresholds}
When \prog{cmsearch} sets its thresholds for each round of the
search, it first sets the final round threshold. This is because the
determination of the filter thresholds depends heavily on the final
threshold. By default, the final round threshold used for calibrated
model searches is $E=0.1$ (see ``manually setting filter and final
thresholds'' for information on how to change this). 
This E-value is then converted to a bit score based on the database
size by using the exponential tail fit parameters stored in the CM
file by \prog{cmcalibrate}. \prog{cmsearch} then sets the HMM filter
threshold and then the QDB filter threshold based on the final round
bit score threshold. In general, the more strict the final round
threshold the more strict the filter thresholds will be. 

\prog{cmcalibrate} automatically determines the appropriate HMM filter
score threshold as described above by generating sequences from the
model and scoring them.  During search, the appropriate threshold is
automatically chosen dependent on the final bit score search threshold
and set as the HMM filter score threshold for that search.
%Importantly, the final bit score threshold is dependent on
%both the final E-value threshold ($0.1$ by default) \emph{and} the
%database size. 
In some cases, it is determined that HMM filtering is a
bad idea because the expected survival fraction is close to $1.0$,
which mean the filter would save little if any time. In these cases,
the HMM filter is turned off. This usually happens when searching
small databases, using low final bit score thresholds (or high final E
value thresholds), or when searching with a model that has very little
primary sequence conservation.

\prog{cmsearch} sets the QDB filter threshold based on the final round
threshold in an \emph{ad hoc} way that seems to work well
empirically. Our goal with the QDB filter is the same as with the HMM
filter, we want the filter to accelerate searches as much as possible
while minimizing sensitivity loss. Our \emph{ad hoc} approach 
sets the QDB filter threshold so that the following two conditions are
met:

\begin{enumerate}
\item
Our QDB filter will always allow hits with E-values of $100$ times our
final E-value threshold to survive. 
\item
The final round of search will require at least $3\%$ of the total
predicted number of dynamic programming (DP) calculations for the
entire search (filters plus final round).
\end{enumerate}

The reasoning here is that the vast majority of hits that will score
below our final round threshold should satisfy condition 1, so
our sensitivity loss from using filter will be small. Empirically we
observe this to be true on internal benchmarks (data not shown).
However, condition 2 states that we're also always willing to perform
at least $3\%$ of our overall number of DP calculations in the final
round. The number of DP calculations in the final round can be
predicted based on the size of the model and on the QDB E-value
cutoff. So given the E-value $x$ that satisfies condition 1, we can
predict the fraction $f(x)$ of the total number of DP calculations that will
occur in the final round. If $f(x)$ is \emph{less} than $0.03$ we can
raise our QDB E-value cutoff to the value $y$ that satisfies $f(y) = 0.03$, and
still meet both of our conditions. In raising the E-value cutoff to
$y$, we're making the filter less strict, which can only reduce our
potential sensitivity loss, but only slowing down the search a small
amount because we've increased the number of calculations by at most
$3\%$.  The value $3\%$ was chosen as a good tradeoff between speed
and sensitivity in our benchmarks. 

One final point: if the predicted survival fraction for the QDB filter
E-value thresold as calculated above is greater than the predicted
survival filter for the HMM filter then \prog{cmsearch} will turn off
the QDB filter mode and use only the HMM filter and the final round
for searching.


\begin{srefaq}{I ran \prog{cmsearch} with a calibrated model, but HMM
    filters were not used for accleration. How come?} This is because
    \prog{cmcalibrate} has determined that it's not ``safe'' to use
    an HMM filter for the search you're running. This means that with
    the final threshold you're using, if you used an HMM filter, 
    to miss no more than 1\% of the hits below your final threshold,
    the filter would have to let nearly the entire database
    survive. In this case \prog{cmsearch} judges it's not worth it to
    use the HMM filter and skips that step of the search. If you
    really want to use an HMM filter you can either (A) make your
    final threshold stricter (\prog{cmstat} can help you determine 
    what effect this will have, see below) or (B) manually set the HMM
    filter threshold as explained in the ``manually setting filter
    and final thresholds'' section.
\end{srefaq}

\begin{srefaq}{I used \prog{cmsearch} twice with the same model and
    the same E-value cutoff on two different databases, but the filter
    thresholds were set differently. Why?} The reason is that the
    databases were different size. The filter thresholds are set based
    on the bit score cutoff of the final round of the search you're
    performing. The bit score is dependent on both the E-value and the
    database size. For example, if you search two databases of size $1$
    Mb and $100$ Mb respectively and use an E-value cutoff of $0.1$ for both
    searches, the bit score cutoff for the 1 Mb database may be $20$
    bits, but it is $27$ bits for the $100$ Mb database. The filters
    can be more strict for the $100$ Mb search because they only have
    to allow all hits that might score above $27$ bits to
    survive instead of all hits that might score above $20$ bits. 
\end{srefaq}

\subsubsection{filtered versus non-filtered search}

Let's see an example of the filters in action. Then we'll compare the
results to a search without using the filters. We'll use a calibrated
CM from the tutorial section, the \prog{purine.2.c.cm} file, a
calibrated Purine riboswitch model, and we'll use it to search the
\emph{Colwellia psycherythraea} genome:

\user{cmsearch purine2.c.cm C.psychrerythraea.genome.fa}

In the tutorial we were only concerned with the hits reported by the
program, but in this section we're interested in the rest of the
output. Let's look at the header and pre-search info section:

\begin{sreoutput}
# command:    cmsearch --forecast 2 my.c.cm C.psychrerythraea.genome.fa
# date:       Wed Apr  2 10:23:58 2008
# dbsize(nt): 10746360
#
# Pre-search info for CM 1: purine.2-1
#
#                                  cutoffs            predictions     
#                            -------------------  --------------------
# rnd  mod  alg  cfg   beta     E value   bit sc     surv     run time
# ---  ---  ---  ---  -----  ----------  -------  -------  -----------
    1  hmm  fwd  loc      -    3837.923    12.05   0.0514  00:02:16.62
    2   cm  cyk  loc  1e-07      89.429    18.88   0.0012  00:00:59.69
    3   cm  ins  loc  1e-15       0.100    37.20  8.9e-07  00:00:15.88
# ---  ---  ---  ---  -----  ----------  -------  -------  -----------
  all    -    -    -      -           -        -        -  00:03:32.20
\end{sreoutput}

First comes the command used and date of execution. Then the database
size is printed in number of nucleotides. Remember, by default,
\prog{cmsearch} searches both strands of the database, so even though
the sequence in \prog{C.psychrerythraea.genome.fa} is only about 5 Mb
long, the database size according to \prog{cmsearch} is twice that,
about 10 Mb. Following that is tabular output section summarizing the
filtering search strategy that \prog{cmsearch} will use in this
search. Here are descriptions of the different columns:

\begin{wideitem}
\item[\emprog{rnd}] the round of searching each row pertains
  to. In this search there's three rounds, two filter rounds plus
  the final round. The hits reported by \prog{cmsearch} are those that
  survive both filters and the final round of searching.

\item[\emprog{mod}] the model each round of searching will use. For
  the first round we'll use a CP9 HMM to filter, for the last two
  rounds the CM is used.

\item[\emprog{alg}] the search algorithm used by each round. 
  The HMM filter round always uses the Forward (``fwd'') HMM algorithm. 
  The QDB filter round always uses the CYK CM algorithm. The final
  round uses the Inside (``ins'') CM algorithm by default, but the 
  CYK algorithm can be used instead with the \prog{--cyk} option. 

\item[\emprog{cfg}] the configuration of the model during each
  round. In this case they are all locally configured. Glocal
  configuration is enabled with the \prog{-g} option (see the tutorial).

\item[\emprog{beta}] the tail loss probability for the QDB
  calculation. This is the amount of probability mass allowed outside
  each band on the DP matrix. For round 2, the QDB filter round, this
  value is 1e-07, so 99.9999\% of the probability mass is within each
  band. For the final round of searching, the bands are less strict,
  with beta values of 1e-15. For more information on this see
  \cite{NawrockiEddy07}.

\item[\emprog{cutoffs}] the cutoffs used for each round of searching,
  in \emprog{E value} and in \prog{bit sc}. Notice how the E-values go
  down and the bit scores go up as you progress through the rounds,
  this means the cutoffs are getting stricter in each successive
  round. 

\item[\emprog{predictions}] the predicted survival fraction
  (\emprog{surv}) and running time (\emprog{run time}) for each
  round. These are based on the E-value cutoffs, and are just
  predictions. A survival fraction of 0.0514 means that our E-values
  predict that 94.86\% of the database will be removed by the HMM
  filter round. We observe these predictions are often inaccurate by up
  to five-fold (and sometimes more), but they are the best predictions
  that we can make; currently.
\end{wideitem}

Look at the pre-search info for this search: the final round (round 3)
E-value cutoff is set as $0.1$ which translates to bit score of $37.2$
in a database of this size. This $0.1$ cutoff is the
default E-value cutoff used by \prog{cmsearch} when searching with a
calibrated model. The HMM filter cutoff is a bit score of $12.05$ bits
which has an E-value of $3838$.  That seems high, but the \prog{surv}
column tells us that if $3838$ hits survive the HMM filter, we'll
filter out about $94.86\%$ of the database. The QDB CYK filter
threshold is $18.88$ bits which has an E-value of about $89$. This
threshold has been set with our \emph{ad hoc} method as a presumably
safe bet to allow any hit that would exceed our final round threshold
of $37.2$ bits with the Inside algorithm to survive. These thresholds
have all been set using the default methods in \prog{cmsearch} for
calibrated models, but if you want, you can change any or all of them
using command-line options as described below in ``Manually setting
filter and final thresholds''.

After a couple of minutes the search finishes, a single hit is output
to the screen (see the tutorial for more on this), and the
``Post-search info'' section is printed:

\begin{sreoutput}
# Post-search info for CM 1: purine.2-1
#
#                              number of hits       surv fraction  
#                            -------------------  -----------------
# rnd  mod  alg  cfg   beta    expected   actual  expected   actual
# ---  ---  ---  ---  -----  ----------  -------  --------  -------
    1  hmm  fwd  loc      -    3837.923      951    0.0514   0.0135
    2   cm  cyk  loc  1e-07      66.795        1    0.0009  1.6e-05
    3   cm  ins  loc  1e-15       0.100        1   8.9e-07  6.2e-06
#
# expected time    actual time
# -------------  -------------
    00:03:26.70    00:02:49.00
\end{sreoutput}

Much of this information we already saw in the pre-search info, except
the columns labelled \emprog{actual}. Our predictions were not very
accurate, but they are within an order of magnitude for the most
part. One striking disparity is the number of actual hits to survive
round 2, the QDB filter round. We predicted that about $67$ hits
would survive, but only $1$ did. One explanation for this is
that the calculation of the expected number ignores the fact that the
HMM filter will be run first, and remove much of the database. In
reality, the QDB filter was only run on $1.35\%$ of the full
database, but the $67$ estimate is based on searching the full
database. (You might think that a better prediction would be to 
multiply our expected QDB hits of $67$ by $0.0514$, our expected
survival fraction from round 1, but this is wrong - the fraction that
survives the HMM filter is not just a random $5.14\%$ of the database,
it's the $5.14\%$ that scores highest with our HMM, and there's no good
theory we can come up with for predicting how many hits we'll get in a
biased fraction of the database like that).

\begin{srefaq}{Why are the \prog{cmsearch} ``run time'' predictions are
  so inaccurate?} Short answer: our E-values are not perfect. In fact,
  they're far from perfect. We're working on making E-values in
  \software{infernal} more accurate and easier to calculate. For now,
  this is the best we can do. Another reason is that for rounds 2 and
  3 of \prog{cmsearch} the search is being performed on the biased
  fraction of the database that has survived all the previous rounds,
  which makes it harder to predict the actual number of hits that will
  survive with accuracy (see above).
\end{srefaq}

So, did our filters do a good job? Well, it's hard to say from what
we've seen. They allowed a single hit through that looks like a real
Purine riboswitch (see the secondary structure figure of the
tutorial). That's good, but who knows, we may have missed 100 others!
And it's impossible to say how much time they've saved us. Well in
this case, the experiment that answers both of these questions is easy
enough to perform, we can run \prog{cmsearch} with the filters turned
off to get the non-filtered running time and to see if anything else
is found. To save you the time of actually doing this we've saved the
output of this search in the file \prog{purine.2.nofilter.cmsearch},
but feel free to run it on your own. Take a look at the file. Here is
the post-search information:

%from
%/groups/eddy/home/nawrockie/notebook/8_0307_inf_uguide_tutorial/filter_dir/purine.2.nofilter.cmsearch, 
% which was run using search.sh in that same dir.

\begin{sreoutput}
# Post-search info for CM 1: purine2-1
#
#                              number of hits       surv fraction  
#                            -------------------  -----------------
# rnd  mod  alg  cfg   beta    expected   actual  expected   actual
# ---  ---  ---  ---  -----  ----------  -------  --------  -------
    1   cm  ins  loc  1e-15       0.100        1   8.9e-07  6.2e-06
#
# expected time    actual time
# -------------  -------------
    03:13:25.22    03:55:36.00
\end{sreoutput}

The non-filtered search only finds this one hit below our E-value
cutoff also. Looks like our filters did a good job in this case!
Notice the time it took to run, about four hours, compared to about three
minutes for our filtered search, so our filters give us an
acceleration of about 80 fold. 

\begin{srefaq}{I just did a search and it took about as long as a
    non-filtered search. How come I didn't see a significant speedup?}
  The speedup does vary a lot depending on the model you're using and
  the size of the database you're searching. \prog{cmcalibrate}
  estimates a HMM filter score cutoff that will accelerate the search
  as much as possible with without losing appreciable
  sensitivity. Sometimes this cutoff is very strict, leading to very
  effective filters, and sometimes it's not. As you might expect, in
  general we find that we can use HMM filters effectively for models
  with regions of high primary sequence similarity, and not so
  effectively for models lacking such regions. You can determine how
  how well an HMM can filter for your model \emph{after} you've
  calibrated it using the \prog{cmstat} program as described below.
\end{srefaq}


\subsubsection{using \prog{cmstat} to get information on HMM filters}

The \prog{cmstat} program can be used to determine how effective an
HMM filter will be for searches with a calibrated CM file. The
\prog{--lfi, --gfi, --lfc} and \prog{--gfc} options will display stats on
HMM filtering for the four different final round search strategies:
local Inside, glocal Inside, local CYK and glocal CYK modes
respectively (by default \prog{cmsearch} uses local Inside as the
final round search strategy). Here's an example:

\user{cmstat --lfi purine.2.c.cm}

\begin{sreoutput}
# local Inside filter threshold stats for E-value cutoff of  0.1 per 1.0000 Mb
#
#  idx  name              clen      cm E  cm bit  hmmbit    surv     xhmm  speedup
# ----  ---------------  -----  --------  ------  ------  ------  -------  -------
     1  purine.2-1         103     0.100    31.3     9.5  0.2781     21.1      3.4
\end{sreoutput}

First, notice the first line says these stats for an E-value cutoff of
0.1 per 1 Mb. This is the default database size that the statistics
are reported for. (You can change this database size using the
\prog{-Z} option, see the manual page on \prog{cmstat}.) What follows
is a tabular description of how effective an HMM filter will be for
this search. Here's what each column means:

\begin{wideitem}

\item[\emprog{idx}] simply the index of the model this row pertains to in
  the CM file. If the input file \prog{purine.2.c.cm} contained
  multiple models, there would be multiple rows in this output, one for
  each model.

\item[\emprog{name}] name of the model.

\item[\emprog{clen}] consensus length of the model

\item[\emprog{cm E}] final round CM E-value cutoff these stats pertain
  to.

\item[\emprog{cm bit}] the bit score the E-value corresponds to in a
  database of size 1 Mb.

\item[\emprog{hmm bit}] the HMM filter bit score threshold that would
  be used for this search.

\item[\emprog{surv}] the \textit{predicted} survival fraction of the database
  that would survive the HMM filter.

\item[\emprog{xhmm}] the \emph{predicted} ratio of millions of dynamic programming (DP)
  calculations required for an HMM filtered search using the cutoff
  given in the ``hmmbit'' column to the millions of DP calcs required
  by a search with only the HMM. This means our filtered search
  (that's the HMM filter plus the CM Inside scan of the predicted
  27.81\% surviving fraction of the database) should take
  roughly 20 times the amount of time an HMM only search would take.
  
\item[\emprog{speedup}] the \textit{predicted} speedup of the HMM filtered
  search versus a non-filtered scan with just the CM Inside algorithm.

\end{wideitem}

This data is really only informative if we're going to do a search of
a 1Mb database. If we're curious about our speedup on a given
database, we can do that with the \prog{--seqfile <f>} option to
\prog{cmstat}. For example to predict the speedup from our previous
search: 

\user{cmstat --lfi --seqfile C.psychrerythraea.genome.fa purine.2.c.cm}

\begin{sreoutput}
# local Inside filter threshold stats for E-value cutoff of  0.1 per 10.7464 Mb
#
#  idx  name              clen      cm E  cm bit  hmmbit    surv     xhmm  speedup
# ----  ---------------  -----  --------  ------  ------  ------  -------  -------
     1  purine.2-1         103     0.100    37.2    12.0  0.0514      4.7     15.3
\end{sreoutput}

Notice the speed-up is significantly more than the 3.4X for the 1Mb
database search. This is because our E-value of 0.1 now corresponds to
a bit score cutoff of 37.2 bits, this is a stricter cutoff so our
filter can be stricter.\prog{cmstat} can also print info on the
predicted speedup for the Rfam cutoff values with the \prog{--ga,
--nc, --tc} options, but only if they're annotated in the CM file
(which ours does not have). 

\prog{cmstat} can also display information on the full range of
possible speedups for different CM bit score cutoffs using the
\prog{--range} option: 

\user{cmstat --lfi --range purine.2.c.cm}

\begin{sreoutput}
# local Inside filter threshold stats for all filter cutoffs in CM file
#
#
#  idx  name              clen       F     nseq  db (Mb)  always?
# ----  ---------------  -----  ------  -------  -------  -------
     1  purine.2-1         103  0.9900    10000      1.0       no
#
#
#       CM E-value cutoff / HMM Forward E-value filter cutoff pairs:
#
#       idx         cm E  cm bit       hmm E  hmmbit    surv     xhmm  speedup
#       ----  ----------  ------  ----------  ------  ------  -------  -------
           1       0.200    29.6    3221.180     8.8  0.4635     34.4      2.1
           2       0.183    29.8    2842.650     8.9  0.4090     30.5      2.4
           3       0.158    30.2    2525.430     9.1  0.3634     27.2      2.7
           4       0.134    30.6    2215.290     9.3  0.3188     24.0      3.0
           5       0.115    31.0    1932.870     9.5  0.2781     21.1      3.4
           6       0.092    31.5    1739.130     9.7  0.2502     19.1      3.8
           7       0.079    31.9    1550.230     9.8  0.2231     17.1      4.2
           8       0.070    32.2    1390.180    10.0  0.2000     15.4      4.7
           9       0.047    33.2    1248.330    10.2  0.1796     14.0      5.2
          10       0.041    33.6    1108.900    10.4  0.1596     12.5      5.8
          11       0.032    34.1     978.046    10.5  0.1407     11.2      6.5
          12       0.025    34.8     854.502    10.7  0.1230      9.9      7.3
          13       0.022    35.0     737.141    11.0  0.1061      8.7      8.3
          14       0.019    35.4     659.813    11.1  0.0949      7.8      9.2
          15       0.017    35.8     568.726    11.4  0.0818      6.9     10.5
          16       0.014    36.1     503.911    11.5  0.0725      6.2     11.6
          17       0.012    36.5     447.678    11.7  0.0644      5.6     12.8
          18       0.011    36.9     400.923    11.9  0.0577      5.2     14.0
          19       0.010    37.0     357.137    12.0  0.0514      4.7     15.3
          20    8.43e-03    37.4     316.435    12.2  0.0455      4.3     16.8
          21    7.34e-03    37.8     283.577    12.4  0.0408      3.9     18.3
          22    6.46e-03    38.1     253.452    12.6  0.0365      3.6     19.9
          23    5.41e-03    38.5     221.313    12.8  0.0318      3.3     21.9
          24    3.98e-03    39.3     198.842    12.9  0.0286      3.1     23.5
          25    3.22e-03    39.8     173.841    13.1  0.0250      2.8     25.7
          26    2.94e-03    40.1     155.800    13.3  0.0224      2.6     27.6
          27    2.46e-03    40.5     139.799    13.5  0.0201      2.5     29.4
          28    2.11e-03    40.9     124.198    13.6  0.0179      2.3     31.5
          29    1.64e-03    41.5     106.011    13.9  0.0153      2.1     34.3
          30    1.52e-03    41.7      96.327    14.0  0.0139      2.0     36.1
          31    7.35e-06    54.9      81.947    14.3  0.0118      1.9     39.0
          32    5.90e-06    55.4      64.894    14.6  0.0093      1.7     43.1
          33    2.86e-06    57.2      17.112    16.6  0.0025      1.2     61.3
          34    2.82e-06    57.2      13.993    16.9  0.0020      1.1     63.0
          35    2.39e-06    57.6      12.136    17.1  0.0017      1.1     64.1
          36    2.38e-06    57.7       9.633    17.5  0.0014      1.1     65.6
\end{sreoutput}

Notice that the output is formatted slightly differently than
before. The first section of tabular output has four new columns, all
containing information from the run of \prog{cmcalibrate} that was
used to calibrate this model: 

\begin{wideitem}

\item[\emprog{F}] this is the fraction of hits \prog{cmcalibrate}
  required the HMM to be able to recognize during filter threshold
  calculation. By default this number is 0.99 as I mentioned above,
  but can be changed with the \prog{-F} option to \prog{cmcalibrate}
  (see the manual page).

\item[\emprog{nseq}] this is the number of sequences generated in \prog{cmcalibrate}
  and searched with the CM and HMM during filter threshold
  calculation. In this case, 10,000 sequences were generated and
  searched, and the HMM must have been able to recognize 99\% of the
  sequences above any given CM score threshold. 

\item[\emprog{db (Mb)}] the database size the data in the lower
  tabular section corresponds to. This is only relevant for the
  E-values in that second section, the bit scores would stay constant
  regardless of the database size.

\item[\emprog{always?}] \prog{cmcalibrate} determines if it is always
  possible to use an HMM filter for the model it is calibrating, no
  matter what the database size and final round CM E value cutoff
  is. If it is, this column reads ``yes'', if not it reads ``no''.
\end{wideitem}

\subsubsection{using \prog{cmstat} to predict running times for
  filtered searches with Rfam cutoffs}
%Section X explains how \prog{cmcalibrate} determines HMM filter
%thresholds to use to accelerate \prog{cmsearch}. These filter
%thresholds are dependent on the final threshold used in
%\prog{cmsearch}, in general, the stricter the final threshold the
%stricter the filter threshold and the greater the acceleration from
%the filter. 
Section X describes how the Rfam curators set GA, NC and TC bit score
cutoffs for each Rfam model. Because these cutoffs are relatively
strict it is often possible to achieve large speedups of up to
100-fold or more when using an Rfam cutoff as the final threshold. The
\prog{cmstat} can print these predicted speedups with the \prog{--ga,
  --nc} and \prog{--tc} options. Here's an example, using the
\prog{rfam10.c.cm} CM file that includes 10 randomly chosen Rfam
8.1 models, that have been calibrated with default parameters to
cmcalibrate:

\user{cmstat --ga --lfi rfam10.c.cm}

\begin{sreoutput}
# local Inside filter threshold stats for Rfam GA gathering cutoff from CM file
#
#  idx  name              clen      cm E  cm bit  hmmbit    surv     xhmm  speedup
# ----  ---------------  -----  --------  ------  ------  ------  -------  -------
     1  Retroviral_psi     118  1.19e-04    27.0     6.5  0.0324      2.9     20.1
     2  snoR43              73  1.17e-07    40.0    10.2  0.0016      1.1     58.6
     3  snoMe28S-U3344      82  1.15e-10    50.0     9.9  0.0015      1.1     61.5
     4  SNORD36             79  3.07e-04    28.0    11.1  0.0014      1.1     62.9
     5  snoMe18S-Um1356     86  8.43e-11    46.0    10.0  0.0015      1.1     60.0
     6  SNORND104           70  2.13e-06    35.0    10.5  0.0017      1.1     54.7
     7  HgcC               130  2.11e-04    24.0    10.9  0.0043      2.0    115.9
     8  SL1                103  1.97e-04    30.0     7.7  0.0166      2.0     30.2
     9  sroH               161  1.45e-12    50.0     8.6  0.0075      1.5     42.5
    10  snoR9              128  6.77e-11    50.0     9.1  0.0012      1.1     78.3
# ----  ---------------  -----  --------  ------  ------  ------  -------  -------
     -  *Average*          103  8.37e-05    38.0     9.5  0.0070      1.5     58.5
     -  *Total*              -         -       -       -       -      1.6     55.8
\end{sreoutput}

The last two lines that report the average and total statistics are
new, they appear with the \prog{--lfi, --lfc,   --gfi, --gfc} if the
CM file you're running  \prog{cmstat} on includes more than 1 model. 
Notice that we're predict about a 55-fold speedup when searching with
all 10 of these models, which means our CM search would be only about
1.6X slower than an HMM search.

\subsection{Filtered searches with non-calibrated models}

It is possible to run \prog{cmsearch} with models that are not
calibrated. Results from searches with non-calibrated models will not
include E-values and will not be automatically accelerated with
appropriate HMM filter thresholds. The tradeoff is that calibration is
expensive, so skipping it can save you time.  

When using a non-calibrated model \prog{cmsearch} will by default use
an HMM filter with a cutoff threshold of $3.0$ bits, and a QDB filter
with a cutoff threshold of $0.0$ bits, although these thresholds can
be changed using command-line options. Let's walk through a quick
example of default search with a non-calibrated version of the tRNA model
from the tutorial. We'll assume here that you've gone through tutorial
section of the guide and the examples earlier in this section (if not,
this part may not make sense).  First, build a model from the
\prog{tRNA.5.sto} Stockholm file (from the \prog{/tutorial/}
subdirectory of \software{Infernal}). We'll use the \prog{-F} option
which allows us to overwrite the file \prog{my.cm} if it exists.

\user{cmbuild -F my.cm trna.5.sto}

Now, let's redo the search from the tutorial of the 300 Kb database,
but this time our model is not calibrated. First, let's try to use
\prog{--forecast} to predict the run time: 

\user{cmsearch --forecast 1 my.cm tosearch.300Kb.db}

You'll get an error message saying that you can't use
\prog{--forecast} for a non-calibrated model. The reason is because
\prog{cmsearch} has no E-value statistics and thus no good way of
estimating how many hits will survive the HMM filter with a threshold
of $3.0$ bits. 

Let's run the actual search:

\user{cmsearch my.cm tosearch.300Kb.db}

First, the ``Pre-search'' info is printed to the screen, which has
less information than it would if the model were calibrated (see
earlier examples in this section): 

\begin{sreoutput}
# Pre-search info for CM 1: trna.5-1
#
# rnd  mod  alg  cfg   beta  bit sc cut
# ---  ---  ---  ---  -----  ----------
    1  hmm  fwd  loc      -        3.00
    2   cm  cyk  loc  1e-07        0.00
    3   cm  ins  loc  1e-15        0.00
\end{sreoutput}

Then about 350 hits are printed to the screen. There's so many because
our cutoff was set as final cutoff was set as $0.0$ bits. Notice that 
the top hit is still the real tRNA from $101$ to $173$, and it 
has the same bit score as in the search with the calibrated model,
which it must, but there's no E-value reported. In this case, it's
pretty clear that the sequence is a good hit to the model, both
because of the high bit score and because of the primary sequence and
structural similarity apparent in the \prog{cmsearch} alignment. 

After all the hits, the ``Post-search'' information is printed showing
you how many hits survived each round:

\begin{sreoutput}
#
# Post-search info for CM 1: trna.5-1
#
# rnd  mod  alg  cfg   beta  bit sc cut  num hits  surv fract
# ---  ---  ---  ---  -----  ----------  --------  ----------
    1  hmm  fwd  loc      -        3.00      1055      0.1889
    2   cm  cyk  loc  1e-07        0.00       259      0.0607
    3   cm  ins  loc  1e-15        0.00       356      0.0199
#
#    run time
# -----------
     00:00:45
\end{sreoutput}

\begin{srefaq}{My search results say that more hits were found in the
final round than in a previous filter round, how is this possible?}
The reason this can happen is because the filter
rounds detect hits and then extend the boundaries of the hit to
include a short stretch of neighboring residues because
we don't want to rely on the filter's definition of the hit end
points. Some hits in the filter rounds can overlap following the
extension, in which case the hits are merged into one single hit. In the
final round it's possible to find more than one hit in the regions that
were overlapping and merged, so you can get more than one final hit in a
region counted as just one hit in a filter round. This is what happens
in the above example. Notice that even though there's more hits in
round 3 than round 2, the survival fraction is less in round 3 than
round 2. Again, this is because hit boundaries are extended in the
filter round to allow subsequent rounds to refine the endpoints,
whereas hit boundaries are not extended in the final round.
\end{srefaq}

You may be thinking that some of these thresholds are
inappropriate. For example, you may not want to look at all 356 hits
that have a bit score above $0.0$ bits, because most of them are
clearly not tRNAs. You're right, and these default thresholds for
non-calibrated models are usually inappropriate. 
Without E-values it's difficult to automatically predict appropriate
thresholds so \prog{cmsearch} doesn't even try. 
\prog{cmsearch} does have several command-line options that allow
the user to manipulate the thresholds to their liking. This is
described next.

\subsection{Manually setting filter and final thresholds}
We described above how \prog{cmsearch} will automatically pick filter
and final thresholds differently depending on if the query model is
calibrated or not. For calibrated models the HMM filter threshold
that will theoretically maximize speed while maintaining 99\% sensitivity
is chosen (sometimes this means turning HMM filtering off if the
predicted speedup is insignificant). The QDB threshold is set in
a more \emph{ad hoc} way, that works well empirically. For
non-calibrated models, the thresholds are automatically set in a very
simplistic way with a $3.0$ bit threshold for the HMM filter and $0.0$
bit threshold for the QDB filter and final round. 

Regardless of whether you're searching with a calibrated or
non-calibrated model, \prog{cmsearch} offers you several options
manually overriding these automatic choices of thresholds. Here's a list:

{\samepage
Final threshold related options:

\begin{tabular}{lll}
                  &                         &               \\
                  & sets final round        &               \\
option            & threshold to            & requirements  \\ \hline
\prog{-E <x>}     & E-value of \prog{<x>}   & model must be calibrated with \prog{cmcalibrate}\\
\prog{-T <x>}     & bit score of \prog{<x>} & none \\
\prog{--ga}       & Rfam GA bit score       & \verb+#=GF GA+ annotation in model training alignment to \prog{cmbuild} \\
\prog{--tc}       & Rfam TC bit score       & \verb+#=GF TC+ annotation in model training alignment to \prog{cmbuild} \\
\prog{--nc}       & Rfam NC bit score       & \verb+#=GF NC+ annotation in model training alignment to \prog{cmbuild}  \\ \hline
                  &                         &               \\
\end{tabular}
}

{\samepage
HMM filter threshold related options: 

\begin{tabular}{lll}
  &                         &               \\
option                  & effect                  & requirements  \\ \hline
\prog{--fil-no-hmm <x>} & turn HMM filter off     & none \\
\prog{--fil-E-hmm <x>}  & sets threshold as E-value \prog{<x>} & model must be calibrated \\
\prog{--fil-T-hmm <x>}  & sets threshold as bit score \prog{<x>} & none \\
\prog{--fil-Smax-hmm <x>}  & sets maximum allowable E-value threshold as & model must be calibrated \\
                        & that which gives predicted survival fraction of \prog{<x>} &   \\ \hline
                  &                         &               \\
\end{tabular}
}

{\samepage
QDB filter threshold related options: 

\begin{tabular}{lll}
                        &                         &               \\
option                  & effect                  & requirements  \\ \hline
\prog{--fil-no-qdb <x>} & turn QDB filter off     & none \\
\prog{--fil-E-qdb <x>}  & sets threshold as E-value \prog{<x>} & model must be calibrated \\
\prog{--fil-T-qdb <x>}  & sets threshold as bit score \prog{<x>} & none \\ \hline
                  &                         &               \\
\end{tabular}
}

You can use either 0 or 1 of the options from each table in a search. 
The Rfam \prog{--ga, --tc} and \prog{--nc} options are explained more
in section 5.

We'll go through two examples of using these options with our tRNA
model. First, with a calibrated or non-calibrated model to set the HMM
filter cutoff to $5.0$ bits, the QDB filter cutoff to $10.0$ bits and the final
cutoff to $15.0$ bits: 

\user{cmsearch --fil-T-hmm 5 --fil-T-qdb 10 -T 15 my.cm tosearch.300Kb.db} 

If your model is calibrated, you can force the HMM filter to be set to
achieve a specified (predicted) survival fraction with
\prog{--fil-Smax-hmm <x>}. With this option for values of \prog{<x> << 1}
you can expect roughly a \prog{1 / <x>} speedup relative to a
non-filtered search. To try it: 

\user{cmsearch --fil-Smax-hmm 0.001 my.c.cm tosearch.300Kb.db}


\begin{sreoutput}
# Pre-search info for CM 1: trna.5-1
#
#                                  cutoffs            predictions     
#                            -------------------  --------------------
# rnd  mod  alg  cfg   beta     E value   bit sc     surv     run time
# ---  ---  ---  ---  -----  ----------  -------  -------  -----------
    1  hmm  fwd  loc      -       5.472    12.18   0.0010  00:00:05.33
    2   cm  ins  loc  1e-15       0.100    17.96  1.1e-05  00:00:00.55
# ---  ---  ---  ---  -----  ----------  -------  -------  -----------
  all    -    -    -      -           -        -        -  00:00:05.88
#
\end{sreoutput}
\begin{comment}
# Post-search info for CM 1: trna.5-1
#
#                              number of hits       surv fraction  
#                            -------------------  -----------------
# rnd  mod  alg  cfg   beta    expected   actual  expected   actual
# ---  ---  ---  ---  -----  ----------  -------  --------  -------
    1  hmm  fwd  loc      -       5.472        2    0.0010   0.0003
    2   cm  ins  loc  1e-15       0.100        1   1.1e-05   0.0001
#
# expected time    actual time
# -------------  -------------
    00:00:05.88    00:00:06.00
\end{comment}

Notice that the HMM filter round 1 cutoff was set as the E-value 
that corresponds to predicted survival fraction of 0.001.

\begin{comment}
Examples:
\user{cmsearch -T 10 my.cm tosearch.300Kb.db}

Notice the filter rounds were unaffected, all that changes is the
number of hits in the final round:

\begin{sreoutput}
# Post-search info for CM 1: trna.5-1
#
# rnd  mod  alg  cfg   beta  bit sc cut  num hits  surv fract
# ---  ---  ---  ---  -----  ----------  --------  ----------
    1  hmm  fwd  loc      -        3.00      1055      0.1889
    2   cm  cyk  loc  1e-07        0.00       259      0.0607
    3   cm  ins  loc  1e-15       10.00         5      0.0005
#
\end{sreoutput}

cmsearch -T 10 --fil-T-qdb 7  my.cm tosearch.300Kb.db
cmsearch -T 10 --fil-T-qdb 7 --fil-T-hmm 10  my.cm tosearch.300Kb.db

NEED TO FIGURE OUT HOW BEST TO ORGANIZE THIS SECTION: I WANT A SECTION
MANIPULATING THRESHOLDS FOR CALIBRATED CMS ALSO. 

Maybe: 
SubSection: Manually setting filter thresholds for \prog{cmsearch}
subsubsection: calibrated models
subsubsection: non-calibrated models

or
SubSection: Manually setting search filter thresholds with calibrated models
SubSection: Manually setting search filter thresholds with non-calibrated models

or
subsubSection: Manually setting search filter thresholds with calibrated models
subsection Searching with non-calibrated models
subSubSection: Manually setting search filter thresholds with non-calibrated models
 
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%below this is crap
\begin{comment} 
One approach at accelerating search is to filter the database with a
fast low specificity method and only use the the slower, more specific
CM methods on subsequences that survive the filter. The amount of the
database that survives the filter is the survival fraction $S$. The
Rfam curators (as of the 8.1 release) use a BLAST based filter when
they do homology search with CMs, which gives them roughly a 1000 fold
speedup, but comes at an unknown cost to sensitivity.
Zasha Weinberg and Larry Ruzzo introduced HMM filters for CM homology
search \cite{WeinbergRuzzo04, WeinbergRuzzo04b,
  WeinbergRuzzo06}. Briefly, they've introduced two main types of
filters: rigorous filters and maximum-likelihood (ML) filters. The rigorous
filters are guaranteed to find all hits above a preset CM bit score
threshold. ML HMM filters achieve a target survival
fraction $S$, by default $S$ is 0.01, which gives a rough speedup of
100 fold (if the time taken to search with the filter is neglected; 
which is usually a small fraction of the CM search time of the
survival fraction). Weinberg's implementation of rigorous filters are
included in \software{infernal} (see the Install section, but
unfortunately require a piece of software \software{cfsqp} to run that
we are not allowed to distribute).  

We've implemented a variant of Weinberg's ML HMM filters in
\software{infernal} which we call CM plan 9 HMMs or CP9 HMMs (to
differentiate them from \software{HMMER}'s plan 7, P7, HMM
architecture).  CP9 HMMs are explicitly based on Weinberg's ML HMM
formulation. 

\prog{cmsearch} includes a second acceleration strategy that
complements HMM filtering called query-dependent banding (QDB).  QDB
precalculates regions of the CM dynamic programming matrix that have
negligible probability and can safely be ignored to save time. We
previously described QDB in \cit \cite{NawrockiEddy07} to be used as
the primary algorithm in \prog{cmsearch}, but in this version of
\software{infernal} it is also used as a filtering step, as we'll
describe next. 


\subsection{Executive summary}

The \prog{cmsearch} uses up to two types of filters to accelerate
homology search. First, an HMM filter is used to scan the target
database, and any hit to the HMM above a threshold bit score is saved
to be searched with the next round of filtering. The specific bit
score threshold used is dependent on both the CM 

QDB banded dynamic programming approach 
\prog{cmsearch} also implements a banded dynamic programming approach
called query-dependent banding (QDB). 


In the following sections, we'll go over how
\prog{cmsearch} uses CP9 HMMs to accelerate homology search.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The \prog{cmsearch} program is slow. This is because the dynamic
programming algorithms used to search database sequences for hits to a
model scale as $O(L(N^2 log(N))$ for an RNA family of length $N$. Here
are some example running times for CMs for three different families:

\prog{cmsearch} uses filters to accelerate homology search. The main
idea of the filter is as a fast first-pass scan of the database with a
``wide net'', capturing everything that could possibly be a good hit
and eliminating everything that is very unlikely to be a good
hit. After the filter is run, the expensive CM algorithms are run only
on the surviving fraction $S$ from the filter. \prog{cmsearch}
implements a two-tiered filtering approach. The first filter is a 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Determining appropriate filter score thresholds}

A goal of our filtering approach is to sacrifice only a small amount
of sensitivity for the win in speed. Weinberg's rigorous HMM filters
addressed this very nicely, by guaranteeing that all hits above a
certain CM threshold will survive the filter. Our approach takes a
slightly different tact, and relaxes the 100 \% guarantee to a 99\%
probability based on an empirical sampling based approach. That is,
we estimate our HMM filter will cause us to miss 1\% of the this that a
non-filtered search would have found. 

Briefly the idea is this:

%First, we'll discuss the CP9 HMM architecture.

%\subsubsection{in more detail: CM Plan 9 HMMs}

\subsubsection{How \prog{cmsearch} selects a filtering strategy and
  score thresholds}
\end{comment}
