\section{How \texttt{cmsearch} uses filters to accelerate search}

A major limitation to the practical use of CMs is the slow running
time of homology search implementations. \prog{cmsearch} has two
acceleration strategies for alleviating this limitation. In this
section we'll briefly describe these two strategies and how they're
employed within \prog{cmsearch}.

The two acceleration strategies are both filtering approaches.  The
main idea of filtering is to use a fast algorithm to do a first-pass
scan of the database with a ``wide net'', allowing everything that
could possibly be a good hit to survive and eliminating everything
that is very unlikely to be a good hit. After the filter is run, the
expensive CM algorithms are run only on the surviving fraction of the
database.

The first filtering approach is to use an HMM as the fast first-pass
algorithm. HMM filtering for CM homology search was introduced by
Weinberg and Ruzzo
\cite{WeinbergRuzzo04,WeinbergRuzzo04b,WeinbergRuzzo06}. We use a
specific type of HMM archictecture, essentially a reimplementation of
Weinberg and Ruzzo's maximum-likelihood HMMs \cite{WeinbergRuzzo06},
that we call CM plan 9 HMMs or CP9 HMMs (to distinguish them from the
plan 7, or p7 HMMs of the \software{HMMER} software package
http://hmmer.janelia.org). HMMs are unable to model the interactions
between base-paired columns that a CM can model, which makes them less
sensitive and specific for RNA sequence analysis, but in exchange they
are more efficient to compute with, so they're faster than CMs, making
them useful for filtering.

The second filtering approach is a banded dynamic programming
technique called query-dependent banding (QDB). QDB precalculates
regions of the CM dynamic programming matrix that have negligible
probability and can be skipped to save time
\cite{NawrockiEddy07}. This calculation is dependent only on the query
CM itself, it's independent of the database being search, so it only
has to be done once per model. 

\prog{cmsearch} uses both of these filtering strategies in
combination. HMM filtering is faster but less specific than QDB
filtering, so it is used first. The database fraction that survives
the HMM is then searched with the QDB filter. Anything that surives
both filters is reevaluated again using the Inside algorithm, which is
the slowest but most specific algorithm we have, which determines the
final scores of the hits in the database. 

\subsection{Determining appropriate HMM filter score thresholds with \prog{cmcalibrate}}

A goal of our filtering approach is to sacrifice only a small amount
of sensitivity for the win in speed. In other words, we want faster
searches, but only if they won't miss potential homologs that a slow,
non-filtered search would have found.  Weinberg's rigorous HMM filters
addressed this very nicely, by guaranteeing that all hits above a
certain CM threshold will survive the filter
\cite{WeinbergRuzzo04}. Our approach takes a slightly different tact,
and relaxes the 100\% guarantee to a 99\% probability using a
empirical sampling technique. That is, we estimate our HMM filter will
cause us to miss 1\% of the hits that a non-filtered search would have
found. This sampling technique is performed by \prog{cmcalibrate} and
takes advantage of the CM as a generative probabilistic model.
Sequences are generated, or emitted, from the model and searched with
both an expensive CM algorithm (either CYK or Inside) and the faster
HMM Forward search algorithm. The HMM scores are ranked and the HMM
filter threshold is set as the $x$, the 99\% worst score. If we assume
the sample of sequences we drew from the HMM is representative of real
RNA homologs of the family we're modelling, then using an HMM filter
and a cutoff of $x$ will allow 99\% of the real homologs to
survive. Of course, this is a strong and potentially dangerous
assumption to make, but it essentially underlies our entire modelling
approach - we're just assuming our CM is a good 'model' of the family
we're trying to model!  In other words, if this assumption is wrong,
we'll get poor performance for a variety of reasons, and an
inappropriate filter threshold would be a relatively weak concern.  In
the future, as CMs get 'better' due to better parameterization, etc.,
our assumption will become more true, and this filtering strategy will
improve as well. 

When you calibrate a model with \prog{cmcalibrate} the HMM filter
thresholds are calculated using the sampling technique and printed to
the CM file. (HMM filter threshold determination is one of two
purposes of \prog{cmcalibrate}, the other is fitting exponential tails
for E-value calcuation as described in section 5).

\subsection{Determining QDB filter score thresholds} 
WRITE THIS SECTION!


\subsection{Filtered versus non-filtered search}

Let's see an example of the filters in action. Then we'll compare the
results to a search without using the filters. We'll use a calibrated
CM from the tutorial section, the \prog{purine1p2.c.cm} file, a
calibrated Purine riboswitch model, and we'll use it to search the
\emph{Colwellia psycherythraea} genome:

\user{cmsearch purine2.c.cm c.psychrerythraea.genome.fa}

In the tutorial we were only concerned with the hits reported by the
program. In this section we're more interested in the rest of the
output. Let's look at the ``Pre-search info'' section:

\begin{sreoutput}
# Pre-search info for CM 1: purine1p2-1
#
#                                  cutoffs            predictions     
#                            -------------------  --------------------
# rnd  mod  alg  cfg   beta     E value   bit sc     surv     run time
# ---  ---  ---  ---  -----  ----------  -------  -------  -----------
    1  hmm  fwd  loc      -    3837.923    12.05   0.0514  00:02:16.62
    2   cm  cyk  loc  1e-07      66.795    19.62   0.0009  00:00:59.69
    3   cm  ins  loc  1e-15       0.100    37.20  8.9e-07  00:00:10.37
# ---  ---  ---  ---  -----  ----------  -------  -------  -----------
  all    -    -    -      -           -        -        -  00:03:26.70
\end{sreoutput}

This is tabular output section summarizes the filtering search
strategy that \prog{cmsearch} will use in this search. Here are
descriptions of the different columns:

\begin{wideitem}
\item[\emprog{rnd}] the round of searching each row pertains
  to. In this search there's three rounds, two filter rounds plus
  the final round. 

\item[\emprog{mod}] the model each round of searching will use. For
  the first round we'll use a CP9 HMM to filter, for the last two rounds the CM is
  used.

\item[\emprog{alg}] the search algorithm used by each round. 
  The HMM filter round always uses the Forward (``fwd'') HMM algorithm. 
  The QDB filter round always uses the CYK CM algorithm. The final
  round uses the Inside (``ins'') CM algorithm by default, but the 
  CYK algorithm can be used instead with the \prog{--cyk} option. 

\item[\emprog{cfg}] the configuration of the model during each
  round. In this case they are all locally configured. Glocal
  configuration is enabled with the \prog{-g} option (see the tutorial).

\item[\emprog{beta}] the tail loss probability for the QDB
  calculation. This is the amount of probability mass allowed outside
  each band on the DP matrix. For round two, the QDB filter round this
  value is 1e-07, so 99.9999\% of the probability mass is within each
  band. For the final round of searching, the bands are less strict,
  with beta values of 1e-15.

\item[\emprog{cutoffs}] the cutoffs used for each round of searching,
  in \emprog{E-value} and in \prog{bit sc}. Notice how the E-values go
  down and the bit scores go up as you progress through the rounds,
  this means the cutoffs are getting stricter in each successive
  round. 

\item[\emprog{predictions}] the predicted survival fraction
  (\emprog{surv}) and running time (\emprog{run time}) for each
  round. These are based on the E-value cutoffs, and are just
  predictions. A survival fraction of 0.0514 means that our E-values
  predict that 94.86\% of the database will be removed by the HMM
  filter round. We observe these predictions are often inaccurate by up
  to five-fold (and sometimes more), but they are the best predictions
  that we can make; currently.
\end{wideitem}

After a couple of minutes the search finishes, a single hit is output
to the screen (see the tutorial for more on this), and the
``Post-search info'' section is printed:

\begin{sreoutput}
# Post-search info for CM 1: purine1p2-1
#
#                              number of hits       surv fraction  
#                            -------------------  -----------------
# rnd  mod  alg  cfg   beta    expected   actual  expected   actual
# ---  ---  ---  ---  -----  ----------  -------  --------  -------
    1  hmm  fwd  loc      -    3837.923      951    0.0514   0.0135
    2   cm  cyk  loc  1e-07      66.795        1    0.0009  1.6e-05
    3   cm  ins  loc  1e-15       0.100        1   8.9e-07  6.2e-06
#
# expected time    actual time
# -------------  -------------
    00:03:26.70    00:02:49.00
\end{sreoutput}

Much of this information we already saw in the pre-search info, except
the columns labelled \emprog{actual}. Our predictions were not very
accurate, but they are within an order of magnitude for the most
part. One striking disparity is the number of actual hits to survive
round 2, the QDB filter round. We predicted that about 67 hits
would survive, but only 1 did. One explanation for this is
that the calculation of the expected number ignores the fact that the
HMM filter will be run first, and remove much of the database. In
reality, the QDB filter was only run on 1.35\% of the full
database, but the 67 estimate is based on searching the full
database. (You might think that we can do better than this prediction
by multiplying our expected QDB hits of 67 by 0.0514, our expected
survival fraction from round 1, but this is wrong - the fraction that
survives the HMM filter is not just a random 5.14\% of the database,
it's the 5.14\% that scores highest with our HMM, and there's no good
theory we can come up with for predicting how many hits we'll get in a
biased fraction of the database like that).

\begin{srefaq}{How come the \prog{cmsearch} ``run time'' predictions are
  so inaccurate?} Short answer: our E-values are not perfect. In fact,
  they're far from perfect. We're working on making E-values in
  \software{infernal} more accurate and easier to calculate. For now,
  this is the best we can do. Another reason is that for rounds 2 and
  3 of \prog{cmsearch} the search is being performed on the biased
  fraction of the database that has survived all the previous rounds,
  which makes it harder to predict the actual number of hits that will
  survive with accuracy (see above).
\end{srefaq}

So, did our filters do a good job? Well, it's hard to say from what
we've seen. They allowed a single hit through that looks like a real
Purine riboswitch (see the secondary structure figure of the
tutorial), which is good, but
who knows, we may have missed 100 others! And it's impossible to say
how much time they've saved us. Well in this case, the experiment that
answers both of these questions is easy enough to perform, we can run
\prog{cmsearch} with the filters turned off to get the non-filtered
running time and to see if anything else is found. To save you the
time of actually doing this we've saved the output of this search in
the file \prog{purine2.nofilter.cmsearch}, but feel free to run it
on your own. Take a look at the file. Here is the post-search
information:

%from
%/groups/eddy/home/nawrockie/notebook/8_0307_inf_uguide_tutorial/filter_dir/purine1p2.nofilter.cmsearch, 
% which was run using search.sh in that same dir.

\begin{sreoutput}
# Post-search info for CM 1: purine2-1
#
#                              number of hits       surv fraction  
#                            -------------------  -----------------
# rnd  mod  alg  cfg   beta    expected   actual  expected   actual
# ---  ---  ---  ---  -----  ----------  -------  --------  -------
    1   cm  ins  loc  1e-15       0.100        1   8.9e-07  6.2e-06
#
# expected time    actual time
# -------------  -------------
    03:13:25.22    03:55:36.00
\end{sreoutput}

The non-filtered search only finds this one hit below our E-value
cutoff also. Looks like our filters did a good job in this case!
Notice the time it took to run, about four hours, compared to about three
minutes for our filtered search, a speedup of about 80 fold. 

\begin{srefaq}{I just did a search, and it took about as long as a
    non-filtered search. How come I didn't see a significant speedup?}
  The speedup does vary a lot depending on the model you're using and
  the size of the database you're searching. Our goal is to accelerate
  the search as much as possible with an HMM filter without losing
  appreciable sensitivity, and this is exactly what \prog{cmcalibrate}
  estimates. In general, we find that we can use HMM filters very
  effectively for models with regions of high primary sequence
  similarity, and not so effectively for models lacking such
  regions. You can determine how effectively an HMM can filter for
  your model \emph{after} you've calibrated it using the \prog{cmstat}
  program as described below.
\end{srefaq}


\subsubsection{Filter thresholds depend on the final threshold}
I've glossed over an important aspect of filter thresholds;
\prog{cmsearch} sets them dependent on the final round cutoff you're
using for your search. To understand why this happens, imagine that
you're doing two searches for tRNA sequences, the first is in a 1 Mb
genome, and the second is in a 100 Mb genome. Let's say you're using a
final round E-value cutoff of 0.1 for both searches. In the 1 Mb
genome this may correspond to a bit score of 20 bits, while in the 100 Mb
genome the bit score cutoff for a 0.1 E-value is closer to 27
bits. Remember, the goal of our filters is to accelerate the search as
much as possible without losing sensitivity, that is, without missing
any hits with E-values less than 0.1.  So it makes sense that
a filter can be more strict for the 100 Mb search because it only
needs to allow hits above 27 bits to survive, whereas for the 1
Mb search the filter must allow hits above 20 bits to survive. In
general, the more strict you're final reporting score threshold, the
more strict your filter can be.

For HMM filter thresholds, \prog{cmcalibrate} automatically determines
the filter score threshold across the relevant range of all possible
final thresholds for a model. A representative set of these scores are
stored in the CM file and during \prog{cmsearch} the appropriate
threshold is chosen dependent on the final E-value threshold for the
search \emph{and} the database size.

The adjustment of QDB filter thresholds based on the final round
threshold is less well-founded than for HMM filters. \prog{cmsearch}
sets the QDB filter threshold in a complex and convoluted way. The
reason for this is I was unable to develop an elegant and effective
technique, so I opted for a inelegant and effective solution
instead of a elegant and ineffective one. 

The QDB filter threshold E-value of $x$ is set as follows: 

NOT SURE WHAT TO DO HERE!

\subsection{Using \prog{cmstat} to get information on HMM filters}

The \prog{cmstat} program can be used to determine how effective an
HMM filter will be for searches with a calibrated CM file. The
\prog{--lfi, --gfi, --lfc} and \prog{--gfc} options will display stats on
HMM filtering for the four different final round search strategies:
local Inside, glocal Inside, local CYK and glocal CYK modes
respectively (by default \prog{cmsearch} uses local Inside as the
final round search strategy). Here's an example:

\newpage
\user{cmstat --lfi purine.2.c.cm}

\begin{sreoutput}
# local Inside filter threshold stats for E-value cutoff of  0.1 per 1.0000 Mb
#
#  idx  name              clen      cm E  cm bit  hmmbit    surv     xhmm  speedup
# ----  ---------------  -----  --------  ------  ------  ------  -------  -------
     1  purine.2-1         103     0.100    31.3     9.5  0.2781     21.1      3.4
\end{sreoutput}

First, notice the first line says these stats for an E-value cutoff of
0.1 per 1 Mb. This is the default database size that the statistics are
reported for. (You can change this database size by providing a
sequence file with the \prog{--seqfile} option, see the manual page on
\prog{cmstat}.) What follows is a tabular description of how effective
an HMM filter will be for this search. Here's what each column means:

\begin{wideitem}

\item[\emprog{idx}] simply the index of the model this row pertains to in
  the CM file. If the input file \prog{purine1p2.c.cm} contained
  multiple models, there would be multiple rows in this output, one for
  each model.

\item[\emprog{name}] name of the model.

\item[\emprog{clen}] consensus length of the model

\item[\emprog{cm E}] final round CM E-value cutoff these stats pertain
  to.

\item[\emprog{cm bit}] the bit score the E-value corresponds to in a
  database of size 1 Mb.

\item[\emprog{hmm bit}] the HMM filter bit score threshold that would
  be used for this search.

\item[\emprog{surv}] the predicted survival fraction of the database
  that would survive the HMM filter.

\item[\emprog{xhmm}] the \emph{predicted} ratio of millions of dynamic programming (DP)
  calculations required for an HMM filtered search using the cutoff
  given in the ``hmmbit'' column to the millions of DP calcs required
  by a search with only the HMM. This means our filtered search
  (that's the HMM filter plus the CM Inside scan of the predicted
  27.81\% surviving fraction of the database) should take
  roughly 20 times the amount of time an HMM only search would take.
  
\item[\emprog{speedup}] the predicted speedup of the HMM filtered
  search versus a non-filtered scan with just the CM Inside algorithm.

\end{wideitem}

This data is really only informative if we're going to do a search of
a 1Mb database. If we're curious about our speedup on a given
database, we can do that with the \prog{--seqfile <f>} option to
\prog{cmstat}. For example to predict the speedup from our previous
search: 

\user{cmstat --lfi --seqfile C.psychrerythraea.genome.fa purine1p2.c.cm}

\begin{sreoutput}
# local Inside filter threshold stats for E-value cutoff of  0.1 per 10.7464 Mb
#
#  idx  name              clen      cm E  cm bit  hmmbit    surv     xhmm  speedup
# ----  ---------------  -----  --------  ------  ------  ------  -------  -------
     1  purine1p2-1        103     0.100    37.2    12.0  0.0514      4.7     15.3
\end{sreoutput}

Notice the speed-up is significantly more than the 3.4X for the 1Mb
database search. This is because our E-value of 0.1 now corresponds to
a bit score cutoff of 37.2 bits, this is a stricter cutoff so our
filter can be stricter.\prog{cmstat} can also print info on the
predicted speedup for the Rfam cutoff values with the \prog{--ga,
--nc, --tc} options, but only if they're annotated in the CM file
(which ours does not have). 

\prog{cmstat} can also display information on the full range of
possible speedups for all given CM bit score cutoffs using the
\prog{--range} option: 

\user{cmstat --lfi --range purine1p2.c.cm}

\begin{sreoutput}
# local Inside filter threshold stats for all filter cutoffs in CM file
#
#
#  idx  name              clen       F     nseq  db (Mb)  always?
# ----  ---------------  -----  ------  -------  -------  -------
     1  purine1p2-1        103  0.9900    10000      1.0       no
#
#
#       CM E-value cutoff / HMM Forward E-value filter cutoff pairs:
#
#       idx         cm E  cm bit       hmm E  hmmbit    surv     xhmm  speedup
#       ----  ----------  ------  ----------  ------  ------  -------  -------
           1       0.200    29.6    3221.180     8.8  0.4635     34.4      2.1
           2       0.183    29.8    2842.650     8.9  0.4090     30.5      2.4
           3       0.158    30.2    2525.430     9.1  0.3634     27.2      2.7
           4       0.134    30.6    2215.290     9.3  0.3188     24.0      3.0
           5       0.115    31.0    1932.870     9.5  0.2781     21.1      3.4
           6       0.092    31.5    1739.130     9.7  0.2502     19.1      3.8
           7       0.079    31.9    1550.230     9.8  0.2231     17.1      4.2
           8       0.070    32.2    1390.180    10.0  0.2000     15.4      4.7
           9       0.047    33.2    1248.330    10.2  0.1796     14.0      5.2
          10       0.041    33.6    1108.900    10.4  0.1596     12.5      5.8
          11       0.032    34.1     978.046    10.5  0.1407     11.2      6.5
          12       0.025    34.8     854.502    10.7  0.1230      9.9      7.3
          13       0.022    35.0     737.141    11.0  0.1061      8.7      8.3
          14       0.019    35.4     659.813    11.1  0.0949      7.8      9.2
          15       0.017    35.8     568.726    11.4  0.0818      6.9     10.5
          16       0.014    36.1     503.911    11.5  0.0725      6.2     11.6
          17       0.012    36.5     447.678    11.7  0.0644      5.6     12.8
          18       0.011    36.9     400.923    11.9  0.0577      5.2     14.0
          19       0.010    37.0     357.137    12.0  0.0514      4.7     15.3
          20    8.43e-03    37.4     316.435    12.2  0.0455      4.3     16.8
          21    7.34e-03    37.8     283.577    12.4  0.0408      3.9     18.3
          22    6.46e-03    38.1     253.452    12.6  0.0365      3.6     19.9
          23    5.41e-03    38.5     221.313    12.8  0.0318      3.3     21.9
          24    3.98e-03    39.3     198.842    12.9  0.0286      3.1     23.5
          25    3.22e-03    39.8     173.841    13.1  0.0250      2.8     25.7
          26    2.94e-03    40.1     155.800    13.3  0.0224      2.6     27.6
          27    2.46e-03    40.5     139.799    13.5  0.0201      2.5     29.4
          28    2.11e-03    40.9     124.198    13.6  0.0179      2.3     31.5
          29    1.64e-03    41.5     106.011    13.9  0.0153      2.1     34.3
          30    1.52e-03    41.7      96.327    14.0  0.0139      2.0     36.1
          31    7.35e-06    54.9      81.947    14.3  0.0118      1.9     39.0
          32    5.90e-06    55.4      64.894    14.6  0.0093      1.7     43.1
          33    2.86e-06    57.2      17.112    16.6  0.0025      1.2     61.3
          34    2.82e-06    57.2      13.993    16.9  0.0020      1.1     63.0
          35    2.39e-06    57.6      12.136    17.1  0.0017      1.1     64.1
          36    2.38e-06    57.7       9.633    17.5  0.0014      1.1     65.6
\end{sreoutput}

Notice that the output is formatted slightly differently than
before. The first section of tabular output has four new columns, all
containing information from the run of \prog{cmcalibrate} that was
used to calibrate this model: 

\begin{wideitem}

\item[\emprog{F}] this is the fraction of hits \prog{cmcalibrate}
  required the HMM to be able to recognize during filter threshold
  calculation. By default this number is 0.99 as I mentioned above,
  but can be changed with the \prog{-F} option to \prog{cmcalibrate}
  (see the manual page).

\item[\emprog{nseq}] this is the number of sequences generated in \prog{cmcalibrate}
  and searched with the CM and HMM during filter threshold
  calculation. In this case, 10,000 sequences were generated and
  searched, and the HMM must have been able to recognize 99\% of the
  sequences above any given CM score threshold. 

\item[\emprog{db (Mb)}] the database size the data in the lower
  tabular section corresponds to. This is only relevant for the
  E-values in that second section, the bit scores would stay constant
  regardless of the database size.

\item[\emprog{always?}] \prog{cmcalibrate} determines if it is always
  possible to use an HMM filter for the model it is calibrating, no
  matter what the database size and final round CM E value cutoff
  is. If it is, this column reads ``yes'', if not it reads ``no''.
\end{wideitem}

\subsubsection{Using \prog{cmstat} to predict running times for
  filtered searches with Rfam cutoffs}
%Section X explains how \prog{cmcalibrate} determines HMM filter
%thresholds to use to accelerate \prog{cmsearch}. These filter
%thresholds are dependent on the final threshold used in
%\prog{cmsearch}, in general, the stricter the final threshold the
%stricter the filter threshold and the greater the acceleration from
%the filter. 
Section X describes how the Rfam curators set GA, NC and TC bit score
cutoffs for each Rfam model. Because these cutoffs are relatively
strict it is often possible to achieve large speedups of up to
100-fold or more when using an Rfam cutoff as the final threshold. The
\prog{cmstat} can print these predicted speedups with the \prog{--ga,
  --nc} and {--tc} options. Here's an example, using the
\prog{rfam10models.cm} CM file that includes 10 randomly chosen Rfam
8.1 models, that have been calibrated with default parameters to
cmcalibrate:

\user{cmstat --ga --lfi rfam10.c.cm}

\begin{sreoutput}
# local Inside filter threshold stats for Rfam GA gathering cutoff from CM file
#
#  idx  name              clen      cm E  cm bit  hmmbit    surv     xhmm  speedup
# ----  ---------------  -----  --------  ------  ------  ------  -------  -------
     1  Retroviral_psi     118  1.19e-04    27.0     6.5  0.0324      2.9     20.1
     2  snoR43              73  1.17e-07    40.0    10.2  0.0016      1.1     58.6
     3  snoMe28S-U3344      82  1.15e-10    50.0     9.9  0.0015      1.1     61.5
     4  SNORD36             79  3.07e-04    28.0    11.1  0.0014      1.1     62.9
     5  snoMe18S-Um1356     86  8.43e-11    46.0    10.0  0.0015      1.1     60.0
     6  SNORND104           70  2.13e-06    35.0    10.5  0.0017      1.1     54.7
     7  HgcC               130  2.11e-04    24.0    10.9  0.0043      2.0    115.9
     8  SL1                103  1.97e-04    30.0     7.7  0.0166      2.0     30.2
     9  sroH               161  1.45e-12    50.0     8.6  0.0075      1.5     42.5
    10  snoR9              128  6.77e-11    50.0     9.1  0.0012      1.1     78.3
# ----  ---------------  -----  --------  ------  ------  ------  -------  -------
     -  *Average*          103  8.37e-05    38.0     9.5  0.0070      1.5     58.5
     -  *Total*              -         -       -       -       -      1.6     55.8
\end{sreoutput}

The last two lines that report the average and total statistics are
new, they appear with the \prog{--lfi, --lfc,   --gfi, --gfc} if the
CM file you're running  \prog{cmstat} on includes more than 1 model. 
Notice that we're predict about a 55-fold speedup when searching with
all 10 of these models, which means our CM search would be only about
1.6X slower than an HMM search.

\subsubsection{Bypassing calibration by manually setting filter
  thresholds}

WRITE THIS SECTION AND INCLUDE AN EXAMPLE!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%below this is crap
\begin{comment} 
One approach at accelerating search is to filter the database with a
fast low specificity method and only use the the slower, more specific
CM methods on subsequences that survive the filter. The amount of the
database that survives the filter is the survival fraction $S$. The
Rfam curators (as of the 8.1 release) use a BLAST based filter when
they do homology search with CMs, which gives them roughly a 1000 fold
speedup, but comes at an unknown cost to sensitivity.
Zasha Weinberg and Larry Ruzzo introduced HMM filters for CM homology
search \cite{WeinbergRuzzo04, WeinbergRuzzo04b,
  WeinbergRuzzo06}. Briefly, they've introduced two main types of
filters: rigorous filters and maximum-likelihood (ML) filters. The rigorous
filters are guaranteed to find all hits above a preset CM bit score
threshold. ML HMM filters achieve a target survival
fraction $S$, by default $S$ is 0.01, which gives a rough speedup of
100 fold (if the time taken to search with the filter is neglected; 
which is usually a small fraction of the CM search time of the
survival fraction). Weinberg's implementation of rigorous filters are
included in \software{infernal} (see the Install section, but
unfortunately require a piece of software \software{cfsqp} to run that
we are not allowed to distribute).  

We've implemented a variant of Weinberg's ML HMM filters in
\software{infernal} which we call CM plan 9 HMMs or CP9 HMMs (to
differentiate them from \software{HMMER}'s plan 7, P7, HMM
architecture).  CP9 HMMs are explicitly based on Weinberg's ML HMM
formulation. 

\prog{cmsearch} includes a second acceleration strategy that
complements HMM filtering called query-dependent banding (QDB).  QDB
precalculates regions of the CM dynamic programming matrix that have
negligible probability and can safely be ignored to save time. We
previously described QDB in \cit \cite{NawrockiEddy07} to be used as
the primary algorithm in \prog{cmsearch}, but in this version of
\software{infernal} it is also used as a filtering step, as we'll
describe next. 


\subsection{Executive summary}

The \prog{cmsearch} uses up to two types of filters to accelerate
homology search. First, an HMM filter is used to scan the target
database, and any hit to the HMM above a threshold bit score is saved
to be searched with the next round of filtering. The specific bit
score threshold used is dependent on both the CM 

QDB banded dynamic programming approach 
\prog{cmsearch} also implements a banded dynamic programming approach
called query-dependent banding (QDB). 


In the following sections, we'll go over how
\prog{cmsearch} uses CP9 HMMs to accelerate homology search.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The \prog{cmsearch} program is slow. This is because the dynamic
programming algorithms used to search database sequences for hits to a
model scale as $O(L(N^2 log(N))$ for an RNA family of length $N$. Here
are some example running times for CMs for three different families:

\prog{cmsearch} uses filters to accelerate homology search. The main
idea of the filter is as a fast first-pass scan of the database with a
``wide net'', capturing everything that could possibly be a good hit
and eliminating everything that is very unlikely to be a good
hit. After the filter is run, the expensive CM algorithms are run only
on the surviving fraction $S$ from the filter. \prog{cmsearch}
implements a two-tiered filtering approach. The first filter is a 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Determining appropriate filter score thresholds}

A goal of our filtering approach is to sacrifice only a small amount
of sensitivity for the win in speed. Weinberg's rigorous HMM filters
addressed this very nicely, by guaranteeing that all hits above a
certain CM threshold will survive the filter. Our approach takes a
slightly different tact, and relaxes the 100 \% guarantee to a 99\%
probability based on an empirical sampling based approach. That is,
we estimate our HMM filter will cause us to miss 1\% of the this that a
non-filtered search would have found. 

Briefly the idea is this:

%First, we'll discuss the CP9 HMM architecture.

%\subsubsection{in more detail: CM Plan 9 HMMs}

\subsubsection{How \prog{cmsearch} selects a filtering strategy and
  score thresholds}
\end{comment}
